[{"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/tutorial_i.ipynb", "origination_method": "extract_from_library_automatic", "code": "import os\nimport pyro\nfrom pyro.distributions import Bernoulli\nimport torch\n\n# ### Defining a Causal Model\n\n# In[1]:\n\n\nimport os\nimport pyro\nfrom pyro.distributions import Bernoulli\n\nsmoke_test = ('CI' in os.environ)\nnum_samples = 1000 if not smoke_test else 10\nnum_iterations = 1000 if not smoke_test else 10\nn_individuals = 10000 if not smoke_test else 10\n\ndef causal_model(stress_pt, smokes_cpt, cancer_cpt):\nstress = pyro.sample(\"stress\", Bernoulli(stress_pt))\nsmokes = pyro.sample(\"smokes\", Bernoulli(smokes_cpt[stress.long()]))\ncancer = pyro.sample(\"cancer\", Bernoulli(cancer_cpt[stress.long(), smokes.long()]))\n# For now we only return smokes because we need to return a single value for the predictive handler.\n# We need to return smokes because the trace address for `smokes` does not include the interventions.\n# TODO: address this in the future.\nreturn smokes\n# return stress, smokes, cancer\n\n\n# Our `causal_model` takes as input three sets of parameters, here just marginal and conditional probability tables, and returns a sample from the joint distribution over a single individual's *stress*, *smokes*, and *cancer* attributes. For now, let's just choose some parameters by hand, but later we'll see how we can place priors over these parameters to express broader uncertainty and enable Bayesian causal inference.\n\n# In[2]:\n\n\nimport torch\n\nstress_pt = torch.tensor([0.5])\nsmokes_cpt  = torch.tensor([0.2, 0.8])\ncancer_cpt  = torch.tensor([[0.1, 0.15],\n[0.8, 0.85]])\n", "description": "This section defines a simple causal model using Pyro, a probabilistic programming language. The model simulates the causal relationships among stress, smokes, and cancer for an individual based on predefined parameters."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/tutorial_i.ipynb", "origination_method": "extract_from_library_automatic", "code": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport pyro\nfrom pyro.distributions import Bernoulli\nimport torch\n\n# ### Simulating Observational Data with Fixed Parameters\n\n# Fortunately, all causal models written in ChiRho compose freely with the probabilistic programming tooling in Pyro. Without belaboring the details, Pyro provides support for probabilistic modeling and inference with a set of composable *effect handlers*, which can be thought of as context managers that provide nonstandard interpretation of `pyro.sample` statements in Pyro programs. For more background on Pyro and its use of effect handlers, see https://pyro.ai/examples/intro_long.html and https://pyro.ai/examples/effect_handlers.html. Perhaps not surprisingly, ChiRho's main capabilities also make heavy use of similar effect handlers.\n#\n# As an example, let's use Pyro's `plate` effect handler to repeatedly sample from the `causal_model` to approximate the joint distribution for all of the attributes.\n\n# In[3]:\n\n\ndef population_causal_model(n_individuals, stress_pt, smokes_cpt, cancer_cpt):\nwith pyro.plate(\"individuals\", n_individuals, dim=-1):\nreturn causal_model(stress_pt, smokes_cpt, cancer_cpt)\n\n\n\nsmokes_obs = population_causal_model(n_individuals, stress_pt, smokes_cpt, cancer_cpt)\n# stress_obs, smokes_obs, cancer_obs = population_causal_model(n_individuals, stress_pt, smokes_cpt, cancer_cpt)\n\nprint(smokes_obs.shape)\n# print(stress_obs.shape, smokes_obs.shape, cancer_obs.shape)\n\n\n# Let's pretend for a moment that we're only interested in the relationship between whether individuals smoke and how likely they are to get cancer. To understand this relationship a bit better, we can simulate observations from our model.\n\n# In[4]:\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\ndef visualize_proportions_single(prediction, title):\n# TODO: Make this prettier\ncancer = prediction['cancer'][0]\nsmokes = prediction['_RETURN'][0]\n\ndata = torch.concat((smokes.reshape(-1, 1),\ncancer.reshape(-1, 1)), axis=-1)\nframe = pd.DataFrame(data.numpy(),\ncolumns=[\"smokes\", \"cancer\"]).astype(\n\"category\").replace({0:\"0\", 1:\"1\"})\n\nreturn sns.histplot(x='smokes', hue='cancer', multiple=\"dodge\", shrink=0.7, data=frame).set(title=title)\n\npredictive_fixed = pyro.infer.Predictive(population_causal_model, num_samples=1000, return_sites=(\"cancer\", \"_RETURN\"))\nprediction_fixed = predictive_fixed(n_individuals, stress_pt, smokes_cpt, cancer_cpt)\n\nvisualize_proportions_single(prediction_fixed, \"Observational Data - Fixed Parameters\")\n\n", "description": "Simulating observational data with fixed parameters to understand the relationship between smoking and cancer occurrence in a population. This section uses Pyro's `plate` for vectorized sampling and plots the proportions of cancer occurrence among smokers and non-smokers."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/tutorial_i.ipynb", "origination_method": "extract_from_library_automatic", "code": "import os\nimport pyro\nfrom pyro.distributions import Bernoulli, Beta\nimport torch\n\n# ### Applying an Intervention\n\n# Simulating from our `causal_model` is certainly interesting and informative, but it doesn't exactly tell us what we're interested in. We want to know to what extent smoking *causes* cancer, not just whether smoking is associated with cancer. To answer this question, we can instead sample from a modified version of our model in which individuals are forced to smoke or not smoke, regardless of their level of stress. We can represent these kinds of modifications as **interventions**, and implement them in ChiRho as a special kind of program transformation. Later we'll see how ChiRho helps to automate the application of these kinds of interventions, but to build some intuition let's first walk through what these new programs would look like if we built them by hand instead.\n#\n# Consider the following new Pyro program; `forced_smokes_model`, which closely resembles our original `causal_model` except in how the *smokes* attribute is assigned. Specifically, we've replaced the expression `smokes = pyro.sample(\"smokes\", Bernoulli(smokes_cpt[stress]))` with the expressions `smokes = pyro.deterministic(\"smokes\", smokes_assignment)`, where `smokes_assignment` is now an argument.\n\n# In[6]:\n\n\ndef forced_smokes_model(stress_pt, smokes_cpt, cancer_cpt, smokes_assignment):\nstress = pyro.sample(\"stress\", Bernoulli(stress_pt))\n# smokes no longer depends on stress.\nsmokes = pyro.deterministic(\"smokes\", smokes_assignment)\ncancer = pyro.sample(\"cancer\", Bernoulli(cancer_cpt[stress.int(), smokes.int()]))\n# For now we only return smokes because we need to return a single value for the predictive handler.\n# We need to return smokes because the trace address for `smokes` does not include the interventions.\n# TODO: address this in the future.\nreturn smokes\n# return stress, smokes, cancer\n\ndef population_forced_smokes_model(n_individuals, stress_pt, smokes_cpt, cancer_cpt):\n# Let's imagine that we force half the people to always smoke and half to never smoke.\nsmokes_assignment = torch.tensor([0., 1.]).repeat(n_individuals // 2)\nwith pyro.plate(\"individuals\", n_individuals, dim=-1):\nreturn forced_smokes_model(stress_pt, smokes_cpt, cancer_cpt, smokes_assignment)\n\n\n# ### Simulating Interventional Data with Fixed Parameters\n\n# Let's visualize a simulation from these new programs (compare this to the previous visualization based on the original simulation). First, one run on a barplot, then the proportions obtained in multiple (1k) runs using a kernel density estimate.\n\n# In[7]:\n\n\npredictive_int_fixed = pyro.infer.Predictive(population_forced_smokes_model, num_samples=num_samples, return_sites=(\"cancer\", \"_RETURN\"))\nprediction_int_fixed = predictive_int_fixed(n_individuals, stress_pt, smokes_cpt, cancer_cpt)\n\nvisualize_proportions_single(prediction_int_fixed, \"Intervened Data - Fixed Parameters\")\n\n\n# In[8]:\n\n\nvisualize_proportions_multiple(prediction_int_fixed, \"Interventional Data - Fixed Parameters\")\n\n\n# Using our intervened model we can clearly see that changing the mechanism by which individuals choose whether or not to smoke dramatically changes the joint distribution between smoking and cancer. In our observational model, individuals who smoke are much more likely to get cancer than those who don't smoke. However, in our modified model in which individuals are either forced to smoke or forced not no smoke, the dependence between smoking and cancer nearly disapears.\n#\n# The phenomenon we're observing in this (contrived) example is known as confounding. Here, the reason we see a statistical dependency between smoking and cancer in our original `causal_model` without any interventions is because of the individuals' stress. Given the parameters we happened to choose, individuals who are stressed are more likely to smoke and stress causes an increase in cancer. The confounding effect of stress dissapears in our intervened program, as stress no longer influences whether individuals smoke or not.\n#\n# In causal inference settings we often wish to answer a specific scientific or policy question using our causal models. For example, we may be interested in the **Average Treatment Effect** (ATE), the average difference in the proportion of individuals with cancer between individuals who were forced to smoke and those who were forced to not smoke. This ATE quantity can be expressed as a function of the population of individuals' attributes after applying an intervention.\n\n# In[9]:\n\n\ndef visualize_ATE(prediction, title, position=(0.6, 1)):\n\ncancer = prediction['cancer']\nsmokes = prediction['_RETURN']\n\ncancer_and_smokes = cancer * smokes\ncancer_given_smokes = cancer_and_smokes.sum(1) / smokes.sum(1)\ncancer_and_not_smokes = cancer * (1 - smokes)\ncancer_given_not_smokes = cancer_and_not_smokes.sum(1) / (1 - smokes).sum(1)\n\nate = cancer_given_smokes - cancer_given_not_smokes\n\nax = sns.kdeplot(data=ate.detach().numpy())\nax.set(xlabel='Average Treatment Effect (ATE)', ylabel='Density', title=title)\nreturn ax\n\nvisualize_ATE(prediction_int_fixed, \"Interventional Data - Fixed Parameters\")\n\n", "description": "Applying an intervention to force individuals in the model to smoke or not smoke, and simulating the effects of this intervention on cancer occurrence. This section highlights how causal relationships can be manipulated and studied through interventions."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/tutorial_i.ipynb", "origination_method": "extract_from_library_automatic", "code": "import os\nimport pyro\nfrom pyro.distributions import Bernoulli, Beta\nimport torch\n\n# ### Adding Uncertainty over Model Parameters\n\n# In[12]:\n\n\nfrom pyro.distributions import Beta\n\ndef parameter_prior():\n\nstress_pt = pyro.sample(\"stress_pt\", Beta(1., 1.))\nsmokes_cpt = pyro.sample(\"smokes_cpt\", Beta(torch.ones(2), torch.ones(2)).to_event(1))\ncancer_cpt = pyro.sample(\"cancer_cpt\", Beta(torch.ones(2, 2), torch.ones(2, 2)).to_event(2))\n\nreturn stress_pt, smokes_cpt, cancer_cpt\n\ndef bayesian_population_causal_model(n_individuals):\nstress_pt, smokes_cpt, cancer_cpt = parameter_prior()\nreturn population_causal_model(n_individuals, stress_pt, smokes_cpt, cancer_cpt)\n\npyro.render_model(bayesian_population_causal_model, (n_individuals,))\n\n\n# ### Simulating Observational Data with Uncertain Parameters\n\n# We have now reached a stage where seeing what the observational or an interventional distribution downstream analytically becomes cumbersome (and it is impossible in more complex cases). But again, by sampling multiple times from the model, sampling 'n_individuals' and calculating the proportions of cancer in two groups each time,  we can approximate how our new uncertainty about the parameters propagates to uncertainty about the relationship between smoking and cancer.\n\n# In[13]:\n\n\npredictive_bayesian = pyro.infer.Predictive(bayesian_population_causal_model, num_samples=num_samples, return_sites=(\"cancer\", \"_RETURN\"))\nprediction_bayesian = predictive_bayesian(n_individuals)\nvisualize_proportions_multiple(prediction_bayesian, \"Observational Data - Uncertain Parameters\")\n\n\n# We see that our uniform uncertainty about the parameters propagates downstream, resulting in broad uncertainty about the observed relationship between smoking and cancer.\n\n# ### Simulating Interventional Data with Uncertain Parameters\n\n# Earlier we showed how to extend the `population_causal_model` to the `bayesian_population_causal_model` by simply sampling from the prior over parameters and then calling `population_causal_model` with the sampled parameters. Perhaps not surprisingly, we can also apply this simple recipe with our `population_forced_smokes_model` to construct a `bayesian_population_forced_smokes_model` which also incorporates uncertainty over parameters.\n#\n#\n\n# In[14]:\n\n\ndef bayesian_population_forced_smokes_model(n_individuals):\nstress_pt, smokes_cpt, cancer_cpt = parameter_prior()\nreturn population_forced_smokes_model(n_individuals, stress_pt, smokes_cpt, cancer_cpt)\n\n\n# In[15]:\n\n\npredictive_int_bayesian = pyro.infer.Predictive(bayesian_population_forced_smokes_model, num_samples=num_samples, return_sites=(\"cancer\", \"_RETURN\"))\nprediction_int_bayesian = predictive_int_bayesian(n_individuals)\nvisualize_proportions_multiple(prediction_int_bayesian, \"Interventional Data - Uncertain Parameters\")\n\n\n# In[16]:\n\n\nvisualize_ATE(prediction_int_bayesian, \"Interventional Data - Uncertain Parameters\")\n\n", "description": "Introducing uncertainty over model parameters by defining a Bayesian hierarchical model. This section shows how to simulate observational and interventional data with uncertain parameters, emphasizing the propagation of parameter uncertainty to outcomes."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/tutorial_i.ipynb", "origination_method": "extract_from_library_automatic", "code": "import pyro\nfrom pyro.distributions import Bernoulli, Beta\nimport torch\n\n# ### Adding Uncertainty over Model Structure\n\n# In addition to adding prior distribution over model parameters given a known structure, we can further add uncertainty to our causal assumptions by writing a probabilistic program that selects between multiple probabilistic programs depending on the result of a discrete random choice. Again, we'll see how we can quickly expand the complexity of our uncertain causal assumptions without needing to rewrite our original `causal_model` from scratch, as probabilistic programs in ChiRho are compositional.\n#\n# In the context of our original smoking example, let's pretend that we don't know whether smoking influences cancer, or whether cancer influences smoking. (This might seem somewhat nonsensical now, but it could have been a \"plausible\" hypothesis 70 years ago.) Graphically, this would be represented as uncertainty about the causal edge between 'smokes` and 'cancer' variables. To implement this uncertainty in ChiRho, we first need to write a new program that represents the alternative observational model.\n\n# In[18]:\n\n\ndef alt_causal_model(stress_pt, cancer_cpt, smokes_cpt):\nstress = pyro.sample(\"stress\", Bernoulli(stress_pt))\ncancer = pyro.sample(\"cancer\", Bernoulli(cancer_cpt[stress.int()]))\nsmokes = pyro.sample(\"smokes\", Bernoulli(smokes_cpt[stress.int(), cancer.int()]))\n# For now we only return smokes because we need to return a single value for the predictive handler.\n# We need to return smokes because the trace address for `smokes` does not include the interventions.\n# TODO: address this in the future.\nreturn smokes\n# return stress, smokes, cancer\n\nstress_pt = torch.tensor([0.5])\ncancer_cpt  = torch.tensor([0.2, 0.8])\nsmokes_cpt  = torch.tensor([[0.1, 0.15],\n[0.8, 0.85]])\n\npyro.render_model(alt_causal_model, (stress_pt, cancer_cpt, smokes_cpt))\n\n\n# Again, we can  use Pyro's `plate` effect handler to represent a distribution over a population of individuals and place a prior over parameters for this alternative model to represent our uncertainty about the strength of effects.\n\n# In[19]:\n\n\ndef alt_population_model(n_individuals, stress_pt, cancer_cpt, smokes_cpt):\nwith pyro.plate(\"individuals\", n_individuals, dim=-1):\nreturn alt_causal_model(stress_pt, cancer_cpt, smokes_cpt)\n\ndef alt_parameter_prior():\n\nstress_pt = pyro.sample(\"stress_pt_alt\", Beta(torch.ones(1), torch.ones(1)).to_event(1))\ncancer_cpt = pyro.sample(\"cancer_cpt_alt\", Beta(torch.ones(2), torch.ones(2)).to_event(1))\nsmokes_cpt = pyro.sample(\"smokes_cpt_alt\", Beta(torch.ones(2, 2), torch.ones(2, 2)).to_event(2))\n\nreturn stress_pt, cancer_cpt, smokes_cpt\n\ndef alt_bayesian_population_causal_model(n_individuals):\nstress_pt, cancer_cpt, smokes_cpt = alt_parameter_prior()\n\nreturn alt_population_model(n_individuals, stress_pt, cancer_cpt, smokes_cpt)\n\npyro.render_model(alt_bayesian_population_causal_model, (n_individuals,))\n\n\n# In[20]:\n\n\npredictive_alt_bayesian = pyro.infer.Predictive(alt_bayesian_population_causal_model, num_samples=1000, return_sites=(\"cancer\", \"_RETURN\"))\nprediction_alt_bayesian = predictive_alt_bayesian(n_individuals)\nvisualize_proportions_multiple(prediction_alt_bayesian, \"Observational Data - Uncertain Parameters - Alternative Structure\")\n\n\n# Just from looking at observational samples obtained using this alternative structure, we do not see anything obviously different from the causal model we've used so far. However, differences will arise when we inspect interventional distributions over random variables. In our new alternative candidate model, smoking has no effect on cancer whatsoever, and thus we shouldn't ever see any difference between cancer rates between individuals when individuals are forced to smoke or not smoke.\n#\n# To visualize this point, we can again apply an intervention to this new model and simulate from the transformed model. This time we'll implement the intervention even more succinctly by intervening on the `alt_bayesian_causal_model` directly.\n\n# In[21]:\n\n\ndef alt_bayesian_forced_smokes_model(n_individuals):\nsmokes_assignment = torch.tensor([0., 1.]).repeat(n_individuals // 2)\ntransformed_model = do(alt_bayesian_population_causal_model, {\"smokes\": smokes_assignment})\nreturn transformed_model(n_individuals)\n\n\n# In[22]:\n\n\npredictive_alt_int_bayesian = pyro.infer.Predictive(alt_bayesian_forced_smokes_model, num_samples=1000, return_sites=(\"cancer\", \"_RETURN\"))\nprediction_alt_int_bayesian = predictive_alt_int_bayesian(n_individuals)\nvisualize_proportions_multiple(prediction_alt_int_bayesian, \"Interventional Data - Uncertain Parameters - Alternative Structure\")\n\n\n# In[23]:\n\n\nvisualize_ATE(prediction_alt_int_bayesian, \"Interventional Data - Uncertain Parameters - Alternative Structure\")\n\n", "description": "Adding uncertainty over model structure by defining alternative causal relationships and simulating data to explore different hypotheses about causal structure."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/tutorial_i.ipynb", "origination_method": "extract_from_library_automatic", "code": "from chirho.counterfactual.handlers import TwinWorldCounterfactual\nfrom chirho.observational.handlers import condition\nfrom pyro.poutine import block\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.infer import SVI\nfrom pyro.optim import Adam\nfrom pyro.infer import Trace_ELBO\nimport pyro\nimport torch\n\n# ### Automatically Constructing Counterfactual Worlds - `TwinWorldCounterfactual`\n\n# To make this concrete, we need to use another program transformation provided by ChiRho, the `TwinWorldCounterfactual` effect handler. Intuitively, this new effect handler tells ChiRho that any `pyro.sample` statements that are downstream of an intervention should be sampled twice, once for the observable world without interventions and once for the counterfactual world with interventions applied.\n\n# In[29]:\n\n\nfrom chirho.counterfactual.handlers import TwinWorldCounterfactual\ntwin_model = TwinWorldCounterfactual()(bayesian_randomized_model)\n\n# Sample from both the factual and counterfactual worlds.\ntwin_model(n_individuals)\n\n\n#\n#\n# Using somewhat light and informal mathematical notation for brevity, the tensors returned from the model transformed using the `TwinWorldCounterfactual` handler represent samples from the joint distribution over $P(stress, smokes, cancer, stress_{cf}, smokes_{cf}, cancer_{cf})$, where $stress$, $smokes$, and $cancer$ are the random variables representing individual attributes without an intervention and $stress_{cf}$, $smokes_{cf}$, and $cancer_{cf}$ are the random variables representing individual attributes after applying our intervention.\n#\n# However, as we mentioned before, causal inference is about coming to causal conclusions combining modeling assumptions *and* data, not just modeling assumptions alone. Now that we have a model representing a joint distribution over both observational and interventional data, we can condition our model and apply approximate inference techniques just as we would any other Pyro model. For example, using Pyro's support for stochastic variaitional inference we can obtain an approximate conditional distribution $P(stress_{cf}, smokes_{cf}, cancer_{cf}|stress, smokes, cancer)$. To see how ChiRho enables causal inference, let's first generate some synthetic observational data for $stress$, $smokes$, and $cancer$ in which approximately half of the individuals smoke, and those who do are more likely to have cancer than those who don't.\n\n# ### Generating Synthetic Data\n\n# In[30]:\n\n\nstress_pt = torch.tensor([0.5])\nsmokes_cpt  = torch.tensor([0.3, 0.6])\ncancer_cpt  = torch.tensor([[0.1, 0.3],\n[0.1, 0.4]])\n\npredictive_data = pyro.infer.Predictive(bayesian_population_causal_model, num_samples=1, return_sites=(\"cancer\", \"stress\", \"_RETURN\"))\ndata = predictive_data(n_individuals)\ndata[\"smokes\"] = data.pop(\"_RETURN\").float()\n\n\n# ### Conditioning the Causal Model on Observational Data\n\n# Now that we have synthetic data, we can condition on it inside of the `TwinWorldCounterfactual` handler we saw earlier. This creates a new model `twin_model_conditioned`, that represents the conditional distribution we're interested in, $p(stress_{cf}, smokes_{cf}, cancer_{cf}|stress, smokes, cancer)$.\n\n# In[31]:\n\n\nfrom chirho.observational.handlers import condition\n\ntwin_model_conditioned = TwinWorldCounterfactual()(condition(bayesian_population_forced_smokes_model, data=data))\n\n\n# ### Using Variational Inference for Bayesian Causal Inference\n\n# After using the `TwinWorldCounterfactual` and the `do` handlers, the resulting model can be treated as an ordinary Pyro program. Just as in Pyro, conditioning a model on data means that we can no longer sample directly from the `twin_model_conditioned` model by just running the program; instead we need to run an *approximate inference algorithm*.\n#\n# Because we've used ChiRho's program transformations to construct an ordinary Pyro program, we can use Pyro's built-in support for approximate inference. To see this, let's run stochastic variational inference with a reparameterized Gaussian mean-field variational approximation.\n\n# In[32]:\n\n\nfrom pyro.poutine import block\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.infer import SVI\n\nadam = pyro.optim.Adam({'lr': 0.03})\nelbo = pyro.infer.Trace_ELBO()\n\nguide = AutoNormal(block(twin_model_conditioned, expose=['stress_pt', 'smokes_cpt', 'cancer_cpt']))\n\nsvi = SVI(twin_model_conditioned, guide, adam, loss=elbo)\n\nfor j in range(num_iterations):\n# calculate the loss and take a gradient step\nloss = svi.step(n_individuals)\nif j % 100 == 0:\nprint(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / n_individuals))\n\n\n# ### Simulating Interventional Data from the Approximate Posterior\n\n# We can visualize the resulting posterior distribution over intervened models just as we did before conditioning on data.\n\n# In[33]:\n\n\n# TODO: It's not clear that we're actually conditioning on the data here, as posterior inference returns quite different results for each run.\n# It's possible that the names don't align with ChiRho's internal transformed names. Should be addressed with new indexing.\n\npredictive_int_posterior = pyro.infer.Predictive(bayesian_population_forced_smokes_model, guide=guide, num_samples=1000, return_sites=(\"cancer\", \"_RETURN\"))\nprediction_int_posterior = predictive_int_posterior(n_individuals)\nvisualize_proportions_multiple(prediction_int_posterior, \"Interventional Data - Posterior Distribution\")\n\n\n# In[34]:\n\n\nvisualize_ATE(prediction_int_posterior, \"Interventional Data - Posterior Distribution\")\n\n\n# Using our conceptual diagrams from before, we can see how data informs our updated belief about causal models, which then propagate forward into counterfactual outcomes and causal conclusions. Even though we've only observed data in a world where people choose whether to smoke or not of their own free will, this observed data tells us something about which causal models are plausible. Importantly, this process of mapping data in one world to conclusions in another (e.g. a world in which people are randomly assigned smoking behavior), requires assumptions. When using ChiRho, the models we write encode those assumptions implicitly by how interventions transform them. In subsequent tutorials we'll discuss this subtlety and consideration in more detail. For now, remember that this ability to reduce causal inference to probabilistic inference doesn't come out of thin air.\n\n# ![fig4](figures/Interventional_Posterior.png)\n\n# ## Recap\n\n# Before we conclude this tutorial, let's first recap, and then zoom out thinking a bit more broadly about what it is that we illustrated.\n#\n# **Observation 1**\n#\n# 1. We wrote a causal model using the Pyro probabilistic programming language relating $stress$, $smokes$ and $cancer$, defined as a function of known parameters, and showed how this model can be used to simulate observations for a population of individuals using Pyro's `plate` effect handler.\n# 2. We wrote a modified version of our causal model in which individuals are \"forced\" to smoke or not smoke, modifying the original mechanism that determines whether individuals choose to smoke or not.\n# 3. We showed how this program rewriting can be automated with ChiRho using the `do` program transformation.\n#\n# **Observation 2**\n#\n# 4. Using our causal model defined as a function of known parameters we wrote a Bayesian hierarchical causal model relating $stress$, $smokes$, and $cancer$. We showed how this Bayesian extension induces a distribution over populations of individuals, and the observational relationships between $smokes$ and $cancer$.\n# 5. We applied ChiRho's `do` program transformation to the Bayesian hierarchical causal model, and showed how this induces a distribution over the attributes of individuals who are forced to smoke or not smoke.\n# 6. We wrote an alternative Bayesian hierarchical causal model in which cancer causes individuals to smoke, rather than vice-versa as in our original model. Similar to our original model, this alternative model induced broad uncertainty over the observational relationship between $smokes$ and $cancer$. However, unlike our original model, forcing individuals smoke induced no change in whether individuals got cancer.\n# 7. We wrote a Bayesian hierarchical causal model with uncertainty over parameters and structure, implemented by randomly choosing between our two candidate causal models based on the outcome of a latent discrete random variable. We showed that this model induced a distribution over average treatment effects that had a sharp peak at $ATE=0$.\n#\n# **Observation 3**\n#\n# 8. We showed how to represent a joint distribution over both observational and interventional distributions using ChiRho's `TwinWorldCounterfactual` effect handler.\n# 9. Conditioning our model on (synthetic) observed data, we showed how causal inference problems can be solved using Pyro's extensive support for approximate variational inference.", "description": "Demonstrating causal inference by automatically constructing counterfactual worlds using `TwinWorldCounterfactual`, conditioning the model on observational data, and using variational inference to estimate the posterior distributions of the counterfactual outcomes."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport torch\nimport pyro\nimport torch.nn as nn\nimport pyro.distributions as dist\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom torch import Tensor\n\nfrom pyro.nn import PyroModule, PyroParam, PyroSample\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather, indices_of\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\npyro.clear_param_store()\npyro.set_rng_seed(1234)\npyro.settings.set(module_local_params=True)\n\nsmoke_test = ('CI' in os.environ)\nmax_epochs = 5 if smoke_test else 2000\nnum_samples = 10 if smoke_test else 10000\n# ## Setup\n\n# Here, we install the necessary Pytorch, Pyro, and ChiRho dependencies for this example.\n\n# In[1]:\n\n\nget_ipython().run_line_magic('reload_ext', 'autoreload')\nget_ipython().run_line_magic('pdb', 'off')\n\nimport os\nimport torch\nimport pyro\nimport torch.nn as nn\nimport pyro.distributions as dist\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom torch import Tensor\n\nfrom pyro.nn import PyroModule, PyroParam, PyroSample\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather, indices_of\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\npyro.clear_param_store()\npyro.set_rng_seed(1234)\npyro.settings.set(module_local_params=True)\n\nsmoke_test = ('CI' in os.environ)\nmax_epochs = 5 if smoke_test else 2000", "description": "This section covers the setup required for the examples, including importing necessary libraries and setting initial configurations for the environment and Pyro options."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nDATA_URL = \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\ndf = pd.read_csv(DATA_URL, delimiter=\",\", index_col=0)\ndf = df.drop(\"supp\", axis=1)\n# Load the data\nDATA_URL = \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\ndf = pd.read_csv(DATA_URL, delimiter=\",\", index_col=0)\n\n# See Chapter 23 in http://ecologia.ib.usp.br/curso_r/lib/exe/fetch.php/bie5782:00_curso_avancado:uriarte:gelman_hill2007_data_analysis_using_regression_and_multilevel-hierarchical_models.pdf\n\n# Drop the description of whether the intervention was used as a supplement to teaching or as a replacement for teaching, as it is post-treatment and\n# may introduce collider bias.\ndf = df.drop(\"supp\", axis=1)\n\n\n# In[3]:\n\n", "description": "Loading the dataset from an external source, dropping irrelevant columns, and visualizing pairwise relationships among variables using seaborn's pairplot."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "import pandas as pd\nimport numpy as np\nimport torch\n\nDATA_URL = \"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/ElectricCompany/data/electric.csv\"\ndf = pd.read_csv(DATA_URL, delimiter=\",\", index_col=0)\ndf = df.drop(\"supp\", axis=1)\nd_X = 5\nn_instances = 2\nn_objects = 96\n# Prepare data for SLC models below.\ntreated_df = df[df[\"treatment\"] == 1]\ncontrol_df = df[df[\"treatment\"] == 0]\n\nd_X = 5\nn_instances = 2\nn_objects = 96\n\n# Process covariates with one-hot encoding of categorical \"grade\" covariate\nX_obs = np.ones((n_instances, n_objects, d_X))\nX_obs[0, :, 0]  = control_df[\"pre_test\"].values\nX_obs[0, :, 1:] = pd.get_dummies(control_df[\"grade\"]).values\nX_obs[1, :, 0]  = treated_df[\"pre_test\"].values\nX_obs[1, :, 1:] = pd.get_dummies(treated_df[\"grade\"]).values\n\nT_obs = np.ones((n_instances, n_objects))\nT_obs[0, :] = 0.\n\nY_obs = np.ones((n_instances, n_objects))\nY_obs[0, :] = control_df[\"post_test\"].values\nY_obs[1, :] = treated_df[\"post_test\"].values\n\nX_obs = torch.tensor(X_obs).float()\nT_obs = torch.tensor(T_obs).float()", "description": "Data preparation for the SLC models, including one-hot encoding of categorical variables and converting datasets to Torch Tensors."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "import torch\nfrom pyro.nn import PyroModule, PyroSample\nimport pyro.distributions as dist\n\nfrom torch import Tensor\nclass BayesianLinear(pyro.nn.PyroModule):\n    # Previous code for BayesianLinear\nclass LinearSLC(pyro.nn.PyroModule):\n# Implements a linear structured latent confounder model.\ndef __init__(self, d_X, n_objects, n_instances, d_U):\nsuper().__init__()\nself.d_X = d_X\nself.d_U = d_U\nself.n_objects = n_objects\nself.n_instances = n_instances\n\n# tiny buffers for device/dtype\nself.register_buffer(\"zero\", torch.tensor(0.))\nself.register_buffer(\"one\", torch.tensor(1.))\n\nself.X_mean_model = BayesianLinear(self.d_U, self.d_X)\nself.T_logit_model = BayesianLinear(self.d_X + self.d_U, 1)\nself.Y_mean_model = BayesianLinear(self.d_X + self.d_U, 2)\n\n@PyroSample\ndef X_var(self):\nreturn dist.HalfCauchy(self.one).expand([self.d_X]).to_event(1)\n\n@PyroSample\ndef Y_var(self):\nreturn dist.HalfCauchy(self.one)\n\ndef forward(self):\n# avoid these appearing in plates - kind of gross\nself.X_var, self.Y_var\nself.X_mean_model.weight, self.X_mean_model.bias\nself.T_logit_model.weight, self.T_logit_model.bias\nself.Y_mean_model.weight, self.Y_mean_model.bias\n\n# This structure implicitly assumes that each object has the same number of instances, and vice-versa.\n# That's ok for this data, but would need to change for general SLC settings.\nwith pyro.plate(\"objects\", self.n_objects, dim=-1) as objects:\n\n# Sample object-level latent confounders, U, for each object (e.g. school)\nU = pyro.sample(\"U\", dist.Normal(self.zero, self.one).expand([self.d_U]).to_event(1))\n\nwith pyro.plate(\"instances\", self.n_instances, dim=-2) as instances:\n\n# Sample instance-level covariates, X, treatment, T, and outcome, Y, for each instance (e.g. course)\nX_loc, X_scale_tril = self.X_mean_model(U), torch.diag_embed(self.X_var)\nX = pyro.sample(\"X\", dist.MultivariateNormal(loc=X_loc, scale_tril=X_scale_tril))\n\nT_logit = self.T_logit_model(U, X)[..., 0]\nT = pyro.sample(\"T\", dist.Bernoulli(logits=T_logit))\n\nY_locs = self.Y_mean_model(U, X)\nY_loc = torch.where(T == 0., Y_locs[..., 0], Y_locs[..., 1])\nY = pyro.sample(\"Y\", dist.Normal(loc=Y_loc, scale=self.Y_var))\n", "description": "Definition of a PyroModule for linear structured latent confounder (SLC) models, featuring Bayesian linear regression components."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "import seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nfrom chirho.observational.handlers import condition\n\n# Previous definitions for plot_predictive and required variables\ndef plot_predictive(model, X_obs, T_obs, guide=None):\n\nmodel = condition(data=dict(X=X_obs, T=T_obs))(model)\n\nif guide:\nguide_tr = pyro.poutine.trace(guide).get_trace()\nmodel_tr = pyro.poutine.trace(pyro.poutine.replay(model, trace=guide_tr)).get_trace()\nelse:\nmodel_tr = pyro.poutine.trace(model).get_trace()\n\nY = model_tr.nodes['Y']['value']\n\ndata_copy = df.copy()\ndata_copy[\"source\"] = \"data\"\n\npredictive_copy = df.copy()\npredictive_copy['post_test'] = Y.reshape(-1, 1).detach().numpy()\npredictive_copy[\"source\"] = \"predictive\"\n\nsns.pairplot(predictive_copy.drop(\"pair_id\", axis=1), hue=\"grade\", diag_kind=\"hist\")\n", "description": "Function to plot predictive distributions using seaborn's pairplot and comparing model predictions with actual data."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "import torch\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\n# Earlier code for LinearSLC\n\nclass LinearSLC(pyro.nn.PyroModule):\n    # Previous definition\nclass LinearSLCITE(pyro.nn.PyroModule):\ndef __init__(self, slc_model: LinearSLC):\nsuper().__init__()\nself.slc_model = slc_model\n\ndef forward(self, X_obs, T_obs, Y_obs):\nwith MultiWorldCounterfactual(), \\\ncondition(data=dict(X=X_obs, T=T_obs, Y=Y_obs)), \\\ndo(actions=dict(T=(0., 1.))):\n\nYs = self.slc_model()\nY_treatment = gather(Ys, IndexSet(T={2}))\nY_control = gather(Ys, IndexSet(T={1}))\nreturn pyro.deterministic(\"ITE\", (Y_treatment - Y_control)[..., None, None, :, :], event_dim=2)\n\n\nlinear_slc = LinearSLC(d_X=d_X, n_objects=n_objects, n_instances=n_instances, d_U=2)\nlinear_slc_ite = LinearSLCITE(linear_slc)", "description": "Defining and instantiating a model for computing individual treatment effect (ITE) using counterfactual querying with interventions."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "import torch\nimport pyro\nimport pyro.infer\nimport pytorch_lightning as pl\n\n# Previous definition and instantiation of linear_slc_ite\nclass LightningSVI(pl.LightningModule):\ndef __init__(self, elbo: pyro.infer.elbo.ELBOModule, **optim_params):\nsuper().__init__()\nself.optim_params = dict(optim_params)\nself.elbo = elbo\n\ndef configure_optimizers(self):\nreturn torch.optim.Adam(self.elbo.parameters(), **self.optim_params)\n\ndef training_step(self, batch, batch_idx):\nreturn self.elbo(*batch)\n\n\nguide = pyro.infer.autoguide.AutoNormal(pyro.poutine.block(hide=[\"Y_counterfactual\"])(linear_slc_ite))\nelbo = pyro.infer.Trace_ELBO(num_particles=10, vectorize_particles=True)\nelbo = elbo(linear_slc_ite, guide)\n\n# initialize parameters\nelbo(X_obs, T_obs, Y_obs)\n\n# fit parameters\ntrain_dataset = torch.utils.data.TensorDataset(X_obs, T_obs, Y_obs)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=X_obs.shape[0])\nsvi = LightningSVI(elbo, lr=0.03)  # 0.03\ntrainer = pl.Trainer(max_epochs=max_epochs, log_every_n_steps=1)", "description": "Implementing Gaussian mean field variational inference using Pyro's AutoNormal guide and using PyTorch Lightning for model training."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/slc.ipynb", "origination_method": "extract_from_library_automatic", "code": "import torch\nfrom pyro.infer import Predictive\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\n# Previous code for the definition of plot_predictive and necessary arguments\npredictive = pyro.infer.Predictive(linear_slc_ite, guide=guide, num_samples=num_samples, parallel=False)\nmf_prediction = predictive(X_obs, T_obs, Y_obs)[\"ITE\"]\n\n# Visualize posterior predictive sample", "description": "Visualization of posterior predictive samples after mean field variational inference."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sdid.ipynb", "origination_method": "extract_from_library_automatic", "code": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport os\n# ## Example: California Smoking Cessation\n# As in [1], we analyze the California Smoking Cessation dataset [2] to estimate the effect cigarette taxes had in California. Specifically, in 1989, California passed Proposition 99 which increased cigarette taxes. We will estimate the impact this policy had on cigarette consumption using the California smoking cessation program dataset. This dataset consists of cigarette consumption of 39 states between 1970 to 2000, 38 of which are control units.\n#\n# We start by loading and visualizing the dataset.\n\n# In[2]:\n\n\n# Let's load the data from the author's github\nDATA_URL = \"https://raw.githubusercontent.com/synth-inference/synthdid/master/data/california_prop99.csv\"\ndata = pd.read_csv(DATA_URL, sep=\";\")\n\n# Model below assumes the response is coded as \"y\"\ndata[\"y\"] = data[\"PacksPerCapita\"].values.copy()\n\n# Assign each unit to treatment or control group\ndata[\"in_treatment_group\"] = 0\ntreated_units = data[data[\"treated\"] == 1][\"State\"].unique()\ndata.loc[data[\"State\"].isin(treated_units), \"in_treatment_group\"] = 1\n\n# Must be that in unit_index, the first 1, ... N_co indcs correspond to control units and\n# the remaining N_co + 1, ..., N correspond to treated units. To ensure this, we sort the\n# dataframe by `Year` and `in_treatment_group`\ndata = data.sort_values([\"Year\", \"in_treatment_group\"])\n\n# Assign unique integer ids for each state and each time\ndata[\"unit_index\"] = pd.factorize(data[\"State\"].values)[0]\ndata[\"time_index\"] = pd.factorize(data[\"Year\"].values)[0]\n\n\n# In[3]:\n\n\n# Show first few rows of the dataset\ndata.head()\n\n\n# In[4]:\n\n\nsns.lineplot(\nx=data[data[\"State\"] == \"California\"][\"Year\"],\ny=data[data[\"State\"] == \"California\"][\"PacksPerCapita\"],\nmarker=\"o\",\nlabel='California',\n)\n\n# The shaded area is the plot is the 95% confidence interval around the mean\nsns.lineplot(\nx=data[data[\"State\"] != \"California\"][\"Year\"],\ny=data[data[\"State\"] != \"California\"][\"PacksPerCapita\"],\nmarker=\"o\",\nlabel='Avg. across all other states',\n)\n\nplt.axvline(\n1989, label=\"Proposition 99 Passed\", color=\"black\", linestyle=\"dashed\"\n)\nplt.ylabel(\"Packs per capita\")\nsns.despine()\nplt.legend()", "description": "Loading and visualizing the California Smoking Cessation dataset"}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sdid.ipynb", "origination_method": "extract_from_library_automatic", "code": "import torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.autoguide import AutoNormal\nimport pandas as pd\nDATA_URL = \"https://raw.githubusercontent.com/synth-inference/synthdid/master/data/california_prop99.csv\"\ndata = pd.read_csv(DATA_URL, sep=\";\")\ndata[\"y\"] = data[\"PacksPerCapita\"].values.copy()\ndata[\"in_treatment_group\"] = 0\ntreated_units = data[data[\"treated\"] == 1][\"State\"].unique()\ndata.loc[data[\"State\"].isin(treated_units), \"in_treatment_group\"] = 1\ndata = data.sort_values([\"Year\", \"in_treatment_group\"])\ndata[\"unit_index\"] = pd.factorize(data[\"State\"].values)[0]\ndata[\"time_index\"] = pd.factorize(data[\"Year\"].values)[0]\npyro.render_model = your_render_model_function_here  # Assuming 'render_model' needs to be implemented by the user as it's not a standard Pyro function.\n# ## Causal Query: Counterfactual Prediction\n#\n# In this setting we would like to estimate a counterfactual: had California not raised cigarette taxes, what would have cigarette consumption been?\n#\n# To estimate this effect, we implement a Bayesian analogue of the Synthetic Difference-in-Differences (SDID) estimator proposed in [1].\n\n# In[5]:\n\n\nclass BayesSDID(pyro.nn.PyroModule):\ndef __init__(self, X: pd.DataFrame):\n\"\"\"\nInput:\nX: dataframe with columns \"unit_index\", \"time_index\", \"in_treatment_group\", \"y\", \"treated\"\n\nNotes:\nMust be that in unit_index, the first 1, ... N_co indcs correspond to control units and\nthe remaining N_co + 1, ..., N correspond to treated units\n\"\"\"\n# Check that treated unit index are always larger than control unit index\nassert X[X[\"in_treatment_group\"] == 0][\"unit_index\"].max() < X[X[\"in_treatment_group\"] == 1][\"unit_index\"].min(), \"All treated unit indices must be larger than control unit indices\"\nsuper().__init__()\nself.X = X.copy()\nself.X = self.X.sort_values([\"time_index\", \"time_index\"])\nunits_by_group = self.X[[\"unit_index\", \"in_treatment_group\"]].drop_duplicates()\nself.N = units_by_group.shape[0] # number of units\nself.N_tr = units_by_group[\"in_treatment_group\"].sum()  # number of treated units\nself.N_co = self.N - self.N_tr  # number of control units\nself.T_pre = self.X[self.X[\"treated\"] == 1][\"time_index\"].min()  # number of pre-treatment periods\nself.T_post = self.X[\"time_index\"].max() - self.T_pre + 1  # number of post-treatment periods\nself.T = self.T_pre + self.T_post  # total number of periods\nself.times_by_units = torch.tensor(pd.pivot_table(self.X, values=\"y\", index=\"time_index\", columns=\"unit_index\").values).float()\nself.avg_y_post_treat = self.times_by_units[self.T_pre:, :self.N_co].mean(axis=0)  # average of each control unit over the post-treatment period\nself.y_pre_treat_tr_avg = self.times_by_units[:self.T_pre, self.N_co:].mean(axis=1)\nself.y = torch.tensor(self.X[\"y\"].values)\nself.treated = torch.tensor(self.X[\"treated\"].values)\nself.unit_index = list(self.X[\"unit_index\"].values)\nself.time_index = list(self.X[\"time_index\"].values)\n\ndef _get_module_param(self, param, module_ix):\nif len(param.shape) > 1:\nreturn param[module_ix].squeeze()\nreturn param\n\ndef sample_synthetic_control_weights(self):\nw0 = pyro.sample(\"w0\", dist.Normal(0, 1)) # intercept\nw_co = pyro.sample(\"w_co\", dist.Dirichlet(torch.ones(self.N_co))) # convex combination of control units\nreturn w0, w_co\n\ndef sample_time_weights(self):\nlam_0 = pyro.sample(\"lam_0\", dist.Normal(0, 10)) # intercept\nlam_pre = pyro.sample(\"lam_pre\", dist.Dirichlet(torch.ones(self.T_pre))) # convex combination of time periods\nreturn lam_0, lam_pre\n\ndef sample_response_params(self, prior_scale=10):\n# Intercept, time fixed effects, treatment effect, unit fixed effects\nmu = pyro.sample(\"mu\", dist.Normal(0, prior_scale))\nbeta = pyro.sample(\"beta\", dist.Normal(0, prior_scale).expand((self.T,)).to_event(1))\ntau = pyro.sample(\"tau\", dist.Normal(0, prior_scale))\nalpha = pyro.sample( \"alpha\", dist.Normal(0, prior_scale).expand((self.N,)).to_event(1))\nreturn mu, beta, tau, alpha\n\ndef synthetic_control_unit(self, times_by_units: torch.Tensor, w0: torch.Tensor, w_co: torch.Tensor):\nreturn w0 + times_by_units.mv(w_co)\n\ndef time_control(self, units_by_time: torch.Tensor, lam_0, lam_pre):\nreturn lam_0 + units_by_time.mv(lam_pre)\n\ndef forward(self, **kwargs):\n# Sample synthetic control weights, time weights, response parameters\nw0, w_co = self.sample_synthetic_control_weights()\n_shape_w_tr = list(w_co.shape)\n_shape_w_tr[-1] = self.N_tr\nw_co_tr = torch.cat([w_co, 1 / self.N_tr * torch.ones(_shape_w_tr)], axis=-1) # TODO: this assumes\nlam_0, lam_pre = self.sample_time_weights()\n_shape_lam_post = list(w_co.shape)\n_shape_lam_post[-1] = self.T_post\nlam_pre_post = torch.cat([lam_pre, 1 / self.T_post * torch.ones(_shape_lam_post)], axis=-1) # TODO: this assumes\nmu, beta, tau, alpha = self.sample_response_params()\n\ny_sc = self.synthetic_control_unit(\nself.times_by_units[:self.T_pre, :self.N_co],\nself._get_module_param(w0, 0),\nself._get_module_param(w_co, 0)\n)\n\nwith pyro.plate(\"synthetic_control_weights\", self.T_pre):\npyro.sample(\"y_pre_treat_tr_avg\", dist.Normal(y_sc, 1.0), obs=self.y_pre_treat_tr_avg)\n\n# Time weights likelihood\ny_time = self.time_control(\nself.times_by_units[:self.T_pre, :].T,\nself._get_module_param(lam_0, 0),\nself._get_module_param(lam_pre, 0)\n)\n\nwith pyro.plate(\"time_weights\", self.N_co):\npyro.sample(\"avg_y_post_treat\", dist.Normal(y_time[:self.N_co], 1.0), obs=self.avg_y_post_treat)\n\n# Response likelihood\n# Here we use the copy of module one parameters to response likelihood to change module one\n# gradients\nweights = self._get_module_param(w_co_tr, 1)[self.unit_index] * self._get_module_param(lam_pre_post, 1)[self.time_index]\nf = self._get_module_param(mu, 1) + self._get_module_param(beta, 1)[self.time_index] + self._get_module_param(alpha, 1)[self.unit_index] + self._get_module_param(tau, 1) * self.treated\nwith pyro.plate(\"response\", self.N * self.T):\npyro.sample(\"y\", dist.Normal(f, 1 / weights), obs=self.y)\n\n\n# Let's visualize our Bayesian SDID probabilistic model.\n\n# In[6]:\n\n", "description": "Defining and visualizing a Bayesian model for synthetic difference-in-differences estimation"}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sdid.ipynb", "origination_method": "extract_from_library_automatic", "code": "import torch\nimport pyro\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.infer import Predictive\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef run_svi_inference(model, n_steps=100, verbose=True, lr=.03, vi_family=AutoNormal, guide=None, **model_kwargs):  # Complete definition here.\nDATA_URL = 'https://raw.githubusercontent.com/synth-inference/synthdid/master/data/california_prop99.csv'\ndata = pd.read_csv(DATA_URL, sep=';')\ndata[\"y\"] = data[\"PacksPerCapita\"].values.copy()\ndata[\"in_treatment_group\"] = 0\ntreated_units = data[data[\"treated\"] == 1][\"State\"].unique()\ndata.loc[data[\"State\"].isin(treated_units), \"in_treatment_group\"] = 1\ndata = data.sort_values([\"Year\", \"in_treatment_group\"])\ndata[\"unit_index\"] = pd.factorize(data[\"State\"].values)[0]\ndata[\"time_index\"] = pd.factorize(data[\"Year\"].values)[0]\nnum_samples = 10  # Assuming a smoke test.\nn_steps = 5  # Assuming a smoke test.\nclass BayesSDID(pyro.nn.PyroModule):  # Complete class definition here.\n# ## Effect estimation with ordinary Bayesian inference\n#\n# First, we estimate $\\tau$ (the effect of Proposition 99) by performing joint Bayesian inference over all latents parameters in the model. We report the marginal approximate posterior over $\\tau$.\n\n# In[7]:\n\n\n# Define a helper function to run SVI. (Generally, Pyro users like to have more control over the training process!)\ndef run_svi_inference(model, n_steps=100, verbose=True, lr=.03, vi_family=AutoNormal, guide=None, **model_kwargs):\nif guide is None:\nguide = vi_family(model)\nelbo = pyro.infer.Trace_ELBO()(model, guide)\n# initialize parameters\nelbo(**model_kwargs)\nadam = torch.optim.Adam(elbo.parameters(), lr=lr)\n# Do gradient steps\nfor step in range(1, n_steps + 1):\nadam.zero_grad()\nloss = elbo(**model_kwargs)\nloss.backward()\nadam.step()\nif (step % 1000 == 0) or (step == 1) & verbose:\nprint(\"[iteration %04d] loss: %.4f\" % (step, loss))\nreturn guide\n\n\n# In[8]:\n\n\njoint_guide = run_svi_inference(BayesSDID(data), vi_family=AutoNormal, n_steps=n_steps)\n# Get posterior samples from the joint guide\njoint_samples = Predictive(BayesSDID(data), guide=joint_guide, num_samples=num_samples)()\n\n\n# In[9]:\n\n\n# Plotting utilities\ndef plot_tau(samples, ax):\nsns.kdeplot(samples[\"tau\"], legend=None, ax=ax)\nsns.despine()\nax.set_yticks([])\nax.set_ylabel(\"Posterior density\")\nax.set_xlabel(\"$\\\\tau$\")\n\ndef plot_synthetic_control(model, samples, data, ax):\ntime_by_units = model.times_by_units[:, :model.N_co]\nw0_samples = samples[\"w0\"].flatten()\nw_co_samples = samples[\"w_co\"]\nn_samples = w0_samples.shape[0]\ny_sc_samples = [model.synthetic_control_unit(time_by_units, w0_samples[i], w_co_samples[i].flatten()) for i in range(n_samples)]\ny_sc_samples = torch.stack(y_sc_samples)\n\n# Plot the synthetic control unit with 90% CI and the actual treated unit (California)\nsns.lineplot(x=sorted(data['Year'].unique()), y=y_sc_samples.mean(axis=0), label=\"Synthetic control unit (90% CI)\", ax=ax)\nax.fill_between(\nsorted(data['Year'].unique()),\ntorch.quantile(y_sc_samples, .05, axis=0),\ntorch.quantile(y_sc_samples, .95, axis=0),\nalpha=0.3,\n)\nsns.lineplot(\nx=data[data[\"State\"] == \"California\"][\"Year\"],\ny=data[data[\"State\"] == \"California\"][\"PacksPerCapita\"],\nmarker=\"o\",\nlabel='California',\nax=ax,\n)\n\nax.axvline(\n1989, label=\"Proposition 99 Passed\", color=\"black\", linestyle=\"dashed\"\n)\nax.set_ylabel(\"Packs per capita\")\nsns.despine()\nplt.legend()\n\n\n# In[10]:\n\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nplot_tau(joint_samples, ax=ax[0])\nplot_synthetic_control(BayesSDID(data), joint_samples, data, ax=ax[1])\n\n\n# In[11]:\n\n\n# Posterior summary of of tau", "description": "Performing joint Bayesian inference and visualizing the effect estimation"}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sdid.ipynb", "origination_method": "extract_from_library_automatic", "code": "from pyro.infer.autoguide import AutoNormal\nfrom pyro.infer import Predictive\nfrom chirho.indexed.handlers import IndexPlatesMessenger\nfrom chirho.observational.handlers.cut import SingleStageCut\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\ndef make_cut_model_single(data, module_one_vars):  # Complete definition here.\ndef run_svi_inference(model, n_steps=100, verbose=True, lr=.03, vi_family=AutoNormal, guide=None, **model_kwargs):  # Complete definition and ensure it's before its first use.\nDATA_URL = 'https://raw.githubusercontent.com/synth-inference/synthdid/master/data/california_prop99.csv'\ndata = pd.read_csv(DATA_URL, sep=';')\ndata[\"y\"] = data[\"PacksPerCapita\"].values.copy()\ndata[\"in_treatment_group\"] = 0\ntreated_units = data[data[\"treated\"] == 1][\"State\"].unique()\ndata.loc[data[\"State\"].isin(treated_units), \"in_treatment_group\"] = 1\ndata = data.sort_values([\"Year\", \"in_treatment_group\"])\ndata[\"unit_index\"] = pd.factorize(data[\"State\"].values)[0]\ndata[\"time_index\"] = pd.factorize(data[\"Year\"].values)[0]\nnum_samples = 10  # Assuming a smoke test.\nn_steps = 5  # Assuming a smoke test.\nclass BayesSDID(pyro.nn.PyroModule):  # Ensure this class is defined before this section.\n# ## Robust effect estimation with modular Bayesian inference\n#\n# From the figure above, we see that the estimated synthetic control has non-trivial deviations from California during the pre-treatment period. To robustify our causal effect estimates, we use modular Bayesian inference and compute the \"cut posterior\" for $\\tau$ [3]. Specifically, we define \"module one\" as all observed and latent variables associated with the time and synthetic control weights. We define \"module two\" as the latent variables used to compute the response likelihood.\n#\n# To approximate the cut posterior we use the variational inference objective discussed in [5].\n\n# In[9]:\n\n\ndef make_cut_model_single(data, module_one_vars):\nmodel = BayesSDID(data)\ndef cut_model_single(*args, **kwargs):\nwith IndexPlatesMessenger(), SingleStageCut(module_one_vars):\nmodel(*args, **kwargs)\nreturn cut_model_single\n\nmodule_one_vars = [\"w0\", \"w_co\", \"y_pre_treat_tr_avg\", \"lam_0\", \"lam_pre\", \"avg_y_post_treat\"]\ncut_model_single = make_cut_model_single(data, module_one_vars)\ncut_guide_single = run_svi_inference(cut_model_single, vi_family=AutoNormal, n_steps=n_steps)\ncut_samples_single = Predictive(cut_model_single, guide=cut_guide_single, num_samples=num_samples)()\n\n\n# Below we see that the synthetic control unit estimated from the cut posterior is a better fit to the treated unit (California) during the pre-treatment period\n\n# In[13]:\n\n\n# Flatten to make use plotting utilities\ncut_samples_reshaped = {\n\"tau\": cut_samples_single[\"tau\"][:, 1, ...].squeeze(),\n\"w0\": cut_samples_single[\"w0\"][:, 0, ...],\n\"w_co\": cut_samples_single[\"w_co\"][:, 0, ...]\n}\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nplot_tau(cut_samples_reshaped, ax=ax[0])\nplot_synthetic_control(BayesSDID(data), cut_samples_reshaped, data, ax=ax[1])\n\n\n# In[14]:\n\n\n# Posterior summary of of tau", "description": "Robust effect estimation with modular Bayesian inference using the 'cut posterior'"}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/actual_causality.ipynb", "origination_method": "extract_from_library_automatic", "code": "import os\nimport pandas as pd\nimport torch\nfrom chirho.indexed.ops import IndexSet, gather, indices_of\nfrom chirho.observational.handlers.condition import condition\nfrom chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\nfrom chirho.explainable.handlers import SearchForExplanation\n\nsmoke_test = ('CI' in os.environ)\nnum_samples = 10 if smoke_test else 200\n\n# In[1]:\n\n\nimport os\nimport pandas as pd\nimport pyro\nimport pyro.distributions as dist\nimport pyro.distributions.constraints as constraints\nimport pyro.infer\nimport torch\nfrom chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\nfrom chirho.explainable.handlers import Preemptions, SplitSubsets, SearchForExplanation\nfrom chirho.indexed.ops import IndexSet, gather, indices_of\nfrom chirho.observational.handlers.condition import condition\n\n\nsmoke_test = ('CI' in os.environ)\nnum_samples = 10 if smoke_test else 200\n\n\n# Instead of full enumeration, we will be approximating the answers with sampling. In particular, answering an actual causality query requires investigating the consequences of intervening on all possible witness candidate nodes in all possible combinations thereof to have the values they actually have in a given model. While complete enumeration would work for smaller models, we implement a more general approximate method, which draws random sets of witness nodes multiple times and intervenes on those sampled sets. For smaller models (as the one used in our examples), complete coverage of all possible combinations is easily obtained. For larger models complete enumeration becomes less feasible.\n#\n#\n# An SCM in this context is represented by a ChiRho model, where the exogenous variables are stochastic and introduced using `pyro.sample`, and all the endogenous variables are determined by these, and introduced by `pyro.deterministic` (read on for examples). For simplicity we often assume most of the nodes are binary (this assumption can be weakened, read on for details), and that the nodes are discrete.\n#\n# The key role in this implementation is played by (1) the `SearchForExplanation` handler. It takes `antecedents`, `witnesses`, `consequents`, `antecedent_bias` and `witness_bias` and, roughly  speaking, makes three steps:\n#\n# (A) It randomly intervenes on some of the antecedents (each antecedent node having probability `0.5 - bias` of being intervened on, with non-null bias to prefer smaller antedecedent sets) to have an alternative value (either pre-specified, or randomly selected, depending on whether we pass a list of concrete interventions, or distribution constraints).\n#\n# (B) randomly preempts some of the witnesses intervening on them to have the observed value in all counterfactual worlds (the probability of witness preemption is `0.5 + witness_bias`). The intuition here is that the witness-preempted nodes are the part of the actual context that is assumed to be kept fixed in a given interventional scenario (a sample covers multiple such scenarios).\n#\n# (C) adds sites with `log_probs` tracking whether the counterfactual value of any of the consequents is different from its observed value,  marking cases where it doesn't with an extremely low `log_prob` (and a value negligibly close to 0 otherwise).\n#\n# Since those steps are achieved by adding new sites to the model, the model trace can now be inspected to test for actual causality. In particular, if the `log_prob` of the site added in (C) is very low, then the antecedent is definitely not an actual cause of the consequent, as a given interventional setting does not result in a change to the consequent(s). If it is zero, minimality claims are evaluated by investigating the `log_prob_sum` corresponding to the antecedent preemption sites - by default, bias is set to `.1` to prefer smaller causal sets. All in all, an antecedent set is an actual cause if all its nodes and only its nodes are intervened on in the MAP (wrt. to log probs at play) counterfactual world.\n\n# In[2]:\n\n\n# somewhat boiler-plate sample trace processing, can be skipped by a reader\n\ndef gather_observed(value, antecedents, witnesses):\n_indices = [\ni for i in list(antecedents.keys()) + witnesses if i in indices_of(value, event_dim=0)\n]\n_int_can = gather(\nvalue, IndexSet(**{i: {0} for i in _indices}), event_dim=0,)\nreturn _int_can\n\ndef gather_intervened(value, antecedents, witnesses):\n_indices = [\ni for i in list(antecedents.keys()) + witnesses if i in indices_of(value, event_dim=0)\n]\n_int_can = gather(\nvalue, IndexSet(**{i: {1} for i in _indices}), event_dim=0,)\nreturn _int_can\n\n\ndef get_table(trace, mwc, antecedents, witnesses, consequents):\n\nvalues_table = {}\nnodes = trace.trace.nodes\nwitnesses = [key for key, _ in witnesses.items()]\n\nwith mwc:\n\nfor antecedent_str in antecedents.keys():\n\nobs_ant = gather_observed(nodes[antecedent_str][\"value\"], antecedents, witnesses)\nint_ant = gather_intervened(nodes[antecedent_str][\"value\"], antecedents, witnesses)\n\nvalues_table[f\"{antecedent_str}_obs\"] = obs_ant.squeeze().tolist()\nvalues_table[f\"{antecedent_str}_int\"] = int_ant.squeeze().tolist()\n\napr_ant = nodes[f\"__antecedent_{antecedent_str}\"][\"value\"]\nvalues_table[f\"apr_{antecedent_str}\"] = apr_ant.squeeze().tolist()\n\nvalues_table[f\"apr_{antecedent_str}_lp\"] = nodes[f\"__antecedent_{antecedent_str}\"][\"fn\"].log_prob(apr_ant)\n\nif witnesses:\nfor candidate in witnesses:\nobs_candidate = gather_observed(nodes[candidate][\"value\"], antecedents, witnesses)\nint_candidate = gather_intervened(nodes[candidate][\"value\"], antecedents, witnesses)\nvalues_table[f\"{candidate}_obs\"] = obs_candidate.squeeze().tolist()\nvalues_table[f\"{candidate}_int\"] = int_candidate.squeeze().tolist()\n\nwpr_con = nodes[f\"__witness_{candidate}\"][\"value\"]\nvalues_table[f\"wpr_{candidate}\"] = wpr_con.squeeze().tolist()\n\n\nfor consequent in consequents:\nobs_consequent = gather_observed(nodes[consequent][\"value\"], antecedents, witnesses)\nint_consequent = gather_intervened(nodes[consequent][\"value\"], antecedents, witnesses)\ncon_lp = nodes[f\"__consequent_{consequent}\"]['fn'].log_prob(torch.tensor(1)) #TODO: this feels like a hack\n_indices_lp = [\ni for i in list(antecedents.keys()) + witnesses if i in indices_of(con_lp)]\nint_con_lp = gather(con_lp, IndexSet(**{i: {1} for i in _indices_lp}), event_dim=0,)\n\n\nvalues_table[f\"{consequent}_obs\"] = obs_consequent.squeeze().tolist()\nvalues_table[f\"{consequent}_int\"] = int_consequent.squeeze().tolist()\nvalues_table[f\"{consequent}_lp\"] = int_con_lp.squeeze().tolist()\n\nvalues_df = pd.DataFrame(values_table)\n\nvalues_df.drop_duplicates(inplace=True)\n\nsummands = [col for col in values_df.columns if col.endswith('lp')]\nvalues_df[\"sum_log_prob\"] =  values_df[summands].sum(axis = 1)\nvalues_df.sort_values(by = \"sum_log_prob\", ascending = False, inplace = True)\n\nreturn values_df\n\n\n# In[3]:\n\n\n# this reduces the actual causality check to checking a property of the\n# resulting sums of log probabilities\n# for the antecedent preemption and the consequent differs nodes\n\ndef ac_check(trace, mwc, antecedents, witnesses, consequents):\n\ntable = get_table(trace, mwc, antecedents, witnesses, consequents)\n\nif (list(table['sum_log_prob'])[0]<= -1e8):\nprint(\"No resulting difference to the consequent in the sample.\")\nreturn\n\nwinners = table[table['sum_log_prob'] == table['sum_log_prob'].max()]\n\n\nac_flags = []\nfor _, row in winners.iterrows():\nactive_antecedents = []\nfor antecedent in antecedents:\nif row[f\"apr_{antecedent}\"] == 0:\nactive_antecedents.append(antecedent)\n\nac_flags.append(set(active_antecedents) == set(antecedents))\n\nif not any(ac_flags):\nprint(\"The antecedent set is not minimal.\")\nelse:\nprint(\"The antecedent set is an actual cause.\")\n", "description": "Processing sample trace to evaluate actual causality claims."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/actual_causality.ipynb", "origination_method": "extract_from_library_automatic", "code": "import pyro\nimport pyro.distributions as dist\nimport pyro.distributions.constraints as constraints\nimport pyro.infer\nfrom chirho.observational.handlers.condition import condition\nfrom chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\nfrom chirho.explainable.handlers import SearchForExplanation\nfrom chirho.indexed.ops import IndexSet, gather, indices_of\n# ### Stone-throwing\n\n# Sally and Billy pick up stones and throw them at a bottle. Sally's stone gets there first, shattering the bottle. Both throws are perfectly accurate, so Billy's stone would have shattered the bottle had it not been preempted by Sally\u2019s throw. (see *Actual Causality*, p. 3 and multiple further points at which the example is discussed in the book).\n\n# In[4]:\n\n\n@pyro.infer.config_enumerate\ndef stones_model():\nprob_sally_throws = pyro.sample(\"prob_sally_throws\", dist.Beta(1, 1))\nprob_bill_throws = pyro.sample(\"prob_bill_throws\", dist.Beta(1, 1))\nprob_sally_hits = pyro.sample(\"prob_sally_hits\", dist.Beta(1, 1))\nprob_bill_hits = pyro.sample(\"prob_bill_hits\", dist.Beta(1, 1))\nprob_bottle_shatters_if_sally = pyro.sample(\"prob_bottle_shatters_if_sally\", dist.Beta(1, 1))\nprob_bottle_shatters_if_bill = pyro.sample(\"prob_bottle_shatters_if_bill\", dist.Beta(1, 1))\n\nsally_throws = pyro.sample(\"sally_throws\", dist.Bernoulli(prob_sally_throws))\nbill_throws = pyro.sample(\"bill_throws\", dist.Bernoulli(prob_bill_throws))\n\n\nnew_shp = torch.where(sally_throws == 1,prob_sally_hits, 0.0)\n\nsally_hits = pyro.sample(\"sally_hits\",dist.Bernoulli(new_shp))\n\nnew_bhp = torch.where(\nbill_throws.bool() & (~sally_hits.bool()),\nprob_bill_hits,\ntorch.tensor(0.0),\n)\n\nbill_hits = pyro.sample(\"bill_hits\", dist.Bernoulli(new_bhp))\n\nnew_bsp = torch.where(bill_hits.bool(), prob_bottle_shatters_if_bill,\ntorch.where(sally_hits.bool(),prob_bottle_shatters_if_sally,torch.tensor(0.0),),)\n\nbottle_shatters = pyro.sample(\"bottle_shatters\", dist.Bernoulli(new_bsp))\n\nreturn {\"sally_throws\": sally_throws, \"bill_throws\": bill_throws,  \"sally_hits\": sally_hits,\n\"bill_hits\": bill_hits,  \"bottle_shatters\": bottle_shatters,}\n\nstones_model.nodes = [\"sally_throws\",\"bill_throws\", \"sally_hits\", \"bill_hits\",\"bottle_shatters\",]\n\n\n# In[5]:\n\n\ndef tensorize_observations(observations):\nreturn {k: torch.as_tensor(v) for k, v in observations.items()}\n\nobservations = {\"prob_sally_throws\": 1.0,\n\"prob_bill_throws\": 1.0,\n\"prob_sally_hits\": 1.0,\n\"prob_bill_hits\": 1.0,\n\"prob_bottle_shatters_if_sally\": 1.0,\n\"prob_bottle_shatters_if_bill\": 1.0,\n\"sally_throws\": 1.0, \"bill_throws\": 1.0}\n\nobservations_tensorized = tensorize_observations(observations)\n\n# One way to go is to manually specify a single alternative value\n# which helps if you explicitly want to use a contrastive notion of\n# actual causality\nantecedents = {\"sally_throws\": 0.0}\nantencedent_bias = 0.1\nwitnesses = {\"bill_throws\": constraints.boolean, \"bill_hits\": constraints.boolean}\nconsequents = {\"bottle_shatters\": constraints.boolean}\n\n\n# In[6]:\n\n\nwith MultiWorldCounterfactual() as mwc:\nwith SearchForExplanation(antecedents = antecedents,\nwitnesses = witnesses, consequents = consequents,\nconsequent_scale= 1e-7):\nwith condition(data = observations_tensorized):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr:\nstones_model()\n\n\n# Once we process the sample trace, the table contains all the information we need to evaluate an actual causality claim.\n#\n# For any node it contains:\n# - `<node>_obs` and `<node>_int`, the observed and intervened values of that node.\n#\n# If a node is an antecedent candidate:\n#\n# - `apr_<node>`, which marks if the node has been preempted as an antecedent; if the value is `0`, the antecedent intervention has been applied.\n# - `apr_<node>_lp` the log probability corresponding to the auxiliary antecedent preemption variable. Its tracking is needed to minimize the cause set.\n#\n# Moreover, for witness candidates, the table contains:\n#\n# - `wpr_<node>`, which marks whether a node has been preempted (intervened to have the same counterfactual value as the observed value). Since for the actual causality queries, log probabilities for either value are the same, and can be safely ignored.\n#\n# For any consequent node:\n#\n# - `<consequent>_lp`, which tracks in terms of log probabilities whether the counterfactual value of a consequent node differs from its observed value. If it doesn't, the value is extremely low, `-1e8` by default, and it is 0 otherwise.\n#\n# The table then sums up the relevant log probabilities in `sum_log_prob`, which effectively ranks interventional settings by whether a change to the consequent resulted and by how small the antecedent set is.\n\n# In[7]:\n\n\nprint(antecedents)\nprint(witnesses)\nstones_table = get_table(tr, mwc, antecedents, witnesses, consequents)\ndisplay(stones_table)\n\n\n# In[8]:\n\n\nac_check(tr, mwc, antecedents, witnesses, consequents)\n\n\n# In[9]:\n\n\n# If, more in the spirit of the original definition\n# we want to search through all possible values of the antecedent,\n# we can use a constraint instead of specifying the counterfactual value\n# manually\n\nantecedents = {\"sally_hits\": pyro.distributions.constraints.boolean}\n\nwith MultiWorldCounterfactual() as mwc:\nwith SearchForExplanation(antecedents = antecedents,\nwitnesses = witnesses, consequents = consequents,\nconsequent_scale= 1e-8):\nwith condition(data = observations_tensorized):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr:\nstones_model()\n\n\n# In[10]:\n\n\n# now our samples include some cases where the antecedent intervention\n# was the same as the observed value; this does not change the result,\n# as the __consequent_ log prob is practically -inf in these cases\n\nstones_table = get_table(tr, mwc, antecedents, witnesses, consequents)\ndisplay(stones_table)\n\n\n# In[11]:\n\n\n# the result of the actual causality check is the same as before\n\nac_check(tr, mwc, antecedents, witnesses, consequents)\n\n# since we're dealing with binary antecedents in this notebook,\n# we'll keep using the contrastive notion in what follows\n", "description": "Sally and Billy throw stones example illustrating actual causality claims with variations including searching through possible antecedent values and checking minimal antecedent sets."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/actual_causality.ipynb", "origination_method": "extract_from_library_automatic", "code": "import pyro\nimport pyro.distributions as dist\nimport pyro.infer\nfrom chirho.observational.handlers.condition import condition\nfrom chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\nfrom chirho.explainable.handlers import SearchForExplanation\n\nsmoke_test = ('CI' in os.environ)\nnum_samples = 10 if smoke_test else 200\n\n# ### Forest fire\n\n# In this simplified model, a forest fire was caused by lightning or an arsonist, so we use three endogenous variables, and two exogenous variables corresponding to the two factors. In the conjunctive model,\n# both of the factors have to be present for the fire to start. In the disjunctive model, each of them alone is sufficient.\n\n# In[14]:\n\n\ndef ff_conjunctive():\nu_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\nu_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n\nmatch_dropped = pyro.deterministic(\"match_dropped\",\nu_match_dropped, event_dim=0)\nlightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\nforest_fire = pyro.deterministic(\"forest_fire\", match_dropped.bool() & lightning.bool(),\nevent_dim=0).float()\n\nreturn {\"match_dropped\": match_dropped, \"lightning\": lightning,\n\"forest_fire\": forest_fire}\n\n\n# In[15]:\n\n\ndef ff_disjunctive():\nu_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\nu_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n\nmatch_dropped = pyro.deterministic(\"match_dropped\",\nu_match_dropped, event_dim=0)\nlightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\nforest_fire = pyro.deterministic(\"forest_fire\", match_dropped.bool() | lightning.bool(), event_dim=0).float()\n\nreturn {\"match_dropped\": match_dropped, \"lightning\": lightning,\n\"forest_fire\": forest_fire}\n\n\n# In[16]:\n\n\nantecedents_ff = {\"match_dropped\": 0.0}\nwitnesses_ff = {\"lightning\": constraints.boolean}\nconsequents_ff = {\"forest_fire\": constraints.boolean}\nobservations_ff = tensorize_observations({\"match_dropped\": 1.0, \"lightning\": 1.0})\n\n\n# In[17]:\n\n\nwith MultiWorldCounterfactual() as mwc_ff:\nwith SearchForExplanation(antecedents = antecedents_ff, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_ff,\nconsequents = consequents_ff,\nconsequent_scale= 1e-8):\nwith condition(data = observations_ff):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_ff:\nff_conjunctive()\n\n\n# In[18]:\n\n\n# In the conjunctive model\n# Each of the two factors is a but-for cause\n\nac_check(tr_ff, mwc_ff, antecedents_ff, witnesses_ff, consequents_ff)\n\n\n# In[19]:\n\n\n# In the disjunctive model\n# there still would be fire if there was no lightning\n\nwith MultiWorldCounterfactual() as mwc_ffd:\nwith SearchForExplanation(antecedents = antecedents_ff,\nantecedent_bias= antencedent_bias,\nwitnesses = witnesses_ff,\nconsequents = consequents_ff,\nconsequent_scale = 1e-8):\nwith condition(data = observations_ff):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_ffd:\nff_disjunctive()\n\n\n# In[20]:\n\n\nac_check(tr_ffd, mwc_ffd, antecedents_ff, witnesses_ff, consequents_ff)\n\n\n# In[21]:\n\n\n# in the disjunctive model\n# the actual cause is the composition of the two factors\n\nantecedents_ffd2 = {\"match_dropped\": 0.0, \"lightning\":0.0}\nwitnesses_ffd2 = {}\n\nwith MultiWorldCounterfactual() as mwc_ffd2:\nwith SearchForExplanation(antecedents = antecedents_ffd2, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_ffd2,\nconsequents = consequents_ff,\nconsequent_scale= 1e-8):\nwith condition(data = observations_ff):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_ffd2:\nff_disjunctive()\n\n\n# In[22]:\n\n\nac_check(tr_ffd2, mwc_ffd2, antecedents_ffd2, witnesses_ffd2, consequents_ff)", "description": "Forest fire example demonstrating both conjunctive and disjunctive models of causality."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/actual_causality.ipynb", "origination_method": "extract_from_library_automatic", "code": "import pyro\nimport pyro.distributions as dist\nfrom chirho.indexed.ops import IndexSet\nfrom chirho.observational.handlers.condition import condition\nfrom chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\nfrom chirho.explainable.handlers import SearchForExplanation\n# ### Doctors\n\n# This example illustrates that actual causality is not, in general, transitive. One doctor is responsible for administering the medicine on Monday, and if she does, Bill recovers on Tuesday.\n# Another doctor is reliable and treats Bill on Tuesday if the first doctor failed to do so on Monday. If both doctors treat Bill, he is in `condition1`, dead on Wednesday. Otherwise, he is either healthy on Tuesday (`condition2`) or healthy on Wednesday (`condition3`), or did not receive any treatment and feels worse but is alive on Wednesday (`condition4`).\n#\n# Now suppose Bill did receive treatment on Monday. This is an actual cause of his not receiving treatment on Tuesday, and the latter is an actual cause of his being alive on Wednesday. However, there is nothing that the first doctor could do to cause Bill to be dead on Wednesday.\n\n# In[23]:\n\n\ndef bc_function(mt, tt):\ncondition1 = (mt == 1) & (tt == 1)\ncondition2 = (mt == 1) & (tt == 0)\ncondition3 = (mt == 0) & (tt == 1)\ncondition4 = ~(condition1 | condition2 | condition3)\n\noutput = torch.where(condition1, torch.tensor(3.0), torch.tensor(0.0))\noutput = torch.where(condition2, torch.tensor(0.0), output)\noutput = torch.where(condition3, torch.tensor(1.0), output)\noutput = torch.where(condition4, torch.tensor(2.0), output)\n\nreturn output\n\n\ndef model_doctors():\nu_monday_treatment = pyro.sample(\"u_monday_treatment\", dist.Bernoulli(0.5))\n\nmonday_treatment = pyro.deterministic(\n\"monday_treatment\", u_monday_treatment, event_dim=0\n)\n\ntuesday_treatment = pyro.deterministic(\n\"tuesday_treatment\",\ntorch.logical_not(monday_treatment).float(),\nevent_dim=0,\n)\n\nbills_condition = pyro.deterministic(\n\"bills_condition\",\nbc_function(monday_treatment, tuesday_treatment),\nevent_dim=0,\n)\n\nbill_alive = pyro.deterministic(\n\"bill_alive\", bills_condition.not_equal(3.0).float(), event_dim=0\n)\n\nreturn {\n\"monday_treatment\": monday_treatment,\n\"tuesday_treatment\": tuesday_treatment,\n\"bills_condition\": bills_condition,\n\"bill_alive\": bill_alive,\n}\n\n\n# In[24]:\n\n\nantecedents_doc1 = {\"monday_treatment\": 0.0}\nwitnesses_doc = {}\nconsequents_doc1 = {\"tuesday_treatment\": constraints.boolean}\nobservations_doc = tensorize_observations({\"u_monday_treatment\": 1.0})\n\n\n# In[25]:\n\n\n# The first actual causal link holds\n\nwith MultiWorldCounterfactual() as mwc_doc1:\nwith SearchForExplanation(antecedents = antecedents_doc1, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_doc,\nconsequents = consequents_doc1,\nconsequent_scale= 1e-8):\nwith condition(data = observations_doc):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_doc1:\nmodel_doctors()\n\nac_check(tr_doc1, mwc_doc1, antecedents_doc1, witnesses_doc, consequents_doc1)\n\n\n# In[26]:\n\n\n# So does the second\n\nantecedents_doc2 = {\"tuesday_treatment\": 1.0}\nconsequents_doc2 = {\"bill_alive\": constraints.boolean}\n\n\nwith MultiWorldCounterfactual() as mwc_doc2:\nwith SearchForExplanation(antecedents = antecedents_doc2, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_doc,\nconsequents = consequents_doc2,\nconsequent_scale= 1e-8):\nwith condition(data = observations_doc):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_doc2:\nmodel_doctors()\n\nac_check(tr_doc2, mwc_doc2, antecedents_doc2, witnesses_doc, consequents_doc2)\n\n\n# In[27]:\n\n\n# The third does not, so transitivity fails!\n\nwith MultiWorldCounterfactual() as mwc_doc3:\nwith SearchForExplanation(antecedents = antecedents_doc1, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_doc,\nconsequents = consequents_doc2,\nconsequent_scale= 1e-8):\nwith condition(data = observations_doc):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_doc3:\nmodel_doctors()\n", "description": "Doctors example illustrating the non-transitivity of actual causality through treatment scenarios."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/actual_causality.ipynb", "origination_method": "extract_from_library_automatic", "code": "import pyro\nimport pyro.distributions as dist\nfrom chirho.indexed.ops import IndexSet\nfrom chirho.observational.handlers.condition import condition\nfrom chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\nfrom chirho.explainable.handlers import SearchForExplanation\n\nsmoke_test = ('CI' in os.environ)\nnum_samples = 10 if smoke_test else 200\n\n# ### Friendly fire\n#\n\n#\n# This comes from a causal model developed in a real-life incident investigation, as discussed in the [Incident Reporting using SERAS\u00ae Reporter and SERAS\u00ae Analyst](https://www.causalis.com/90-publications/99-downloads/) paper.\n#\n# A U.S. Special Forces air controller changing the battery on a Global Positioning System device he was using to target a Taliban outpost north of Kandahar. Three special forces soldiers were killed and 20 were injured when a 2,000-pound, satellite-guided bomb landed, not on the Taliban outpost, but on a battalion command post occupied by American forces and a group of Afghan allies, including Hamid Karzai, now the interim prime minister. The Air Force combat controller was using a Precision Lightweight GPS Receiver to calculate the Taliban's coordinates for the attack. The controller did not realize that after he changed the device's battery, the machine was programmed to automatically come back on displaying coordinates for its own location, the official said.\n#\n# Minutes before the B-52 strike, the controller had used the GPS receiver to\n# calculate the latitude and longitude of the Taliban position in minutes and seconds for an airstrike by a Navy F/A-18. Then, with the B-52 approaching the target, the air controller did a second calculation in \u201cdegree decimals\u201d required by the bomber crew.  The controller had performed the calculation and recorded the position, when the receiver battery died. Without realizing the machine was programmed to come back on showing the coordinates of its\n# own location, the controller mistakenly called in the American position to the B-52.\n#\n# Factors included in the model (will be connected in the model as specified in the original report):\n#\n# 1. The air controller changed the battery on the PLGR\n# 2. Three special forces soldiers were killed and 20 were injured\n# 3. B-52 fired a JDAM bomb at the Allied position\n# 4. The air controller was using the PLGR to calculate the Taliban's coordinates\n# 5. The controller did not realize that the PLGR was programmed to automatically come back on displaying coordinates for its own location\n# 6. The controller had used the PLGR to calculate the latitude and longitude of the Taliban position in minutes and seconds for an airstrike by a Navy F/A-18\n# 7. The air controller did a second calculation in \u201cdegree decimals\u201d required by the bomber crew\n# 8. The controller had performed the calculation and recorded the position\n# 9. The controller mistakenly called in the American position to the B-52\n# 10. The B-52 fired a JDAM bomb at the Allied position\n# 11. The U.S. Air Force and Army had a training problem\n# 12. The PLRG resumed displaying the coordinates of its own location after the battery was changed\n# 13. The battery died at the crucial time\n# 14. The controller thought he was calling in the Taliban position\n#\n# We will encode the model and show that in such somewhat more complicated cases, answers to `ac_check` queries are also intuitive.\n\n# In[28]:\n\n\ndef model_friendly_fire():\nu_f4_PLGR_now = pyro.sample(\"u_f4_PLGR_now\", dist.Bernoulli(0.5))\nu_f11_training = pyro.sample(\"u_f11_training\", dist.Bernoulli(0.5))\n\nf4_PLGR_now = pyro.deterministic(\"f4_PLGR_now\", u_f4_PLGR_now, event_dim=0)\nf11_training = pyro.deterministic(\n\"f11_training\", u_f11_training, event_dim=0\n)\n\nf6_PLGR_before = pyro.deterministic(\n\"f6_PLGR_before\", f4_PLGR_now, event_dim=0\n)\nf7_second_calculation = pyro.deterministic(\n\"f7_second_calculation\", f4_PLGR_now, event_dim=0\n)\nf13_battery_died = pyro.deterministic(\n\"f13_battery_died\",\nf6_PLGR_before.bool() & f7_second_calculation.bool(),\nevent_dim=0,\n)\n\nf1_battery_change = pyro.deterministic(\n\"f1_battery_change\", f13_battery_died, event_dim=0\n)\n\nf12_PLGR_after = pyro.deterministic(\n\"f12_PLGR_after\", f1_battery_change, event_dim=0\n)\n\nf5_unaware = pyro.deterministic(\"f5_unaware\", f11_training, event_dim=0)\n\nf14_wrong_position = pyro.deterministic(\n\"f14_wrong_position\", f5_unaware, event_dim=0\n)\n\nf9_mistake_call = pyro.deterministic(\n\"f9_mistake_call\",\nf12_PLGR_after.bool() &\nf14_wrong_position.bool(),\nevent_dim=0,\n)\n\nf3_fired = pyro.deterministic(\"f3_fired\", f9_mistake_call, event_dim=0)\n\nf10_landed = pyro.deterministic(\n\"f10_landed\", f3_fired.bool() &  f9_mistake_call.bool(), event_dim=0\n)\n\nf2_killed = pyro.deterministic(\"f2_killed\", f10_landed, event_dim=0)\n\nreturn {\n\"f1_battery_change\": f1_battery_change,\n\"f2_killed\": f2_killed,\n\"f3_fired\": f3_fired,\n\"f4_PLGR_now\": f4_PLGR_now,\n\"f5_unaware\": f5_unaware,\n\"f6_PLGR_before\": f6_PLGR_before,\n\"f7_second_calculation\": f7_second_calculation,\n\"f9_mistake_call\": f9_mistake_call,\n\"f10_landed\": f10_landed,\n\"f11_training\": f11_training,\n\"f12_PLGR_after\": f12_PLGR_after,\n\"f13_battery_died\": f13_battery_died,\n\"f14_wrong_position\": f14_wrong_position,\n}\n\n\n# In[29]:\n\n\ndef convert_to_bool(list):\nreturn {key: constraints.boolean for key in list}\n\nantecedents_fi1 = {\"f6_PLGR_before\": 0.0, \"f7_second_calculation\": 0.0}\nconsequents_fi = convert_to_bool([\"f2_killed\"])\nwitnesses_fi  = convert_to_bool([\"f4_PLGR_now\",\"f5_unaware\", \"f11_training\", \"f14_wrong_position\"])\nobservations_fi = tensorize_observations({\"u_f4_PLGR_now\": 1.0, \"u_f11_training\": 1.0})\n\n\n# In[30]:\n\n\nwith MultiWorldCounterfactual() as mwc_fi1:\nwith SearchForExplanation(antecedents = antecedents_fi1, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_fi,\nconsequents = consequents_fi,\nconsequent_scale= 1e-8):\nwith condition(data = observations_fi):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_fi1:\nmodel_friendly_fire()\n\n\n# In[31]:\n\n\nac_check(tr_fi1, mwc_fi1, antecedents_fi1, witnesses_fi, consequents_fi)\n\n\n# In[32]:\n\n\nantecedents_fi2 = {\"f6_PLGR_before\": 0.0}\n\nwith MultiWorldCounterfactual() as mwc_fi2:\nwith SearchForExplanation(antecedents = antecedents_fi2, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_fi,\nconsequents = consequents_fi,\nconsequent_scale= 1e-8):\nwith condition(data = observations_fi):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_fi2:\nmodel_friendly_fire()\n\nac_check(tr_fi2, mwc_fi2, antecedents_fi2, witnesses_fi, consequents_fi)\n", "description": "Friendly fire incident modeling showing application of causality analysis in complex real-life situations."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/actual_causality.ipynb", "origination_method": "extract_from_library_automatic", "code": "import pyro\nimport pyro.distributions as dist\nfrom chirho.indexed.ops import IndexSet\nfrom chirho.observational.handlers.condition import condition\nfrom chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\nfrom chirho.explainable.handlers import SearchForExplanation\n\nsmoke_test = ('CI' in os.environ)\nnum_samples = 10 if smoke_test else 200\n\n# ### Voting\n#\n\n# The main reason why the voting models are interesting in this context is that we are interested in the role of particular voters in the coming about of the result. The intuition is that a voter might play are role or be blamed for not voting even if her vote is not decisive. This should be handled by the notion of responsibility. For now, we just notice that the notion of actual causality at play is not enough to capture these intuitions. Say you give one vote in a binary majority vote, `vote0`, you vote \"for\", and there are six other voters.\n\n# In[33]:\n\n\ndef voting_model():\nu_vote0 = pyro.sample(\"u_vote0\", dist.Bernoulli(0.6))\nu_vote1 = pyro.sample(\"u_vote1\", dist.Bernoulli(0.6))\nu_vote2 = pyro.sample(\"u_vote2\", dist.Bernoulli(0.6))\nu_vote3 = pyro.sample(\"u_vote3\", dist.Bernoulli(0.6))\nu_vote4 = pyro.sample(\"u_vote4\", dist.Bernoulli(0.6))\nu_vote5 = pyro.sample(\"u_vote5\", dist.Bernoulli(0.6))\n\nvote0 = pyro.deterministic(\"vote0\", u_vote0, event_dim=0)\nvote1 = pyro.deterministic(\"vote1\", u_vote1, event_dim=0)\nvote2 = pyro.deterministic(\"vote2\", u_vote2, event_dim=0)\nvote3 = pyro.deterministic(\"vote3\", u_vote3, event_dim=0)\nvote4 = pyro.deterministic(\"vote4\", u_vote4, event_dim=0)\nvote5 = pyro.deterministic(\"vote5\", u_vote5, event_dim=0)\n\noutcome = pyro.deterministic(\"outcome\", vote0 + vote1 + vote2 + vote3 + vote4 + vote5 > 3).float()\nreturn {\"outcome\": outcome}\n\n\n# In[34]:\n\n\nantecedents_v = {\"vote0\":0.0}\noutcome_v = convert_to_bool([\"outcome\"])\nwitnesses_v = convert_to_bool([f\"vote{i}\" for i in range(1,6)])\nobservations_v1 = tensorize_observations(dict(u_vote0=1., u_vote1=1., u_vote2=1.,\nu_vote3=1., u_vote4=0., u_vote5=0.))\n\n\n# In[35]:\n\n\nwith MultiWorldCounterfactual() as mwc_v1:\nwith SearchForExplanation(antecedents = antecedents_v, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_v,\nconsequents = outcome_v,\nconsequent_scale= 1e-8):\nwith condition(data = observations_v1):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_v1:\nvoting_model()\n\n# if you're one of four voters who voted for, you are an actual cause\n# of the outcome\n\nac_check(tr_v1, mwc_v1, antecedents_v, witnesses_v, outcome_v)\n\n\n# In[36]:\n\n\n# if you're one of five voters who voted for, you are not an actual cause\n# of the outcome\n\nobservations_v2 = tensorize_observations(dict(u_vote0=1., u_vote1=1., u_vote2=1.,\nu_vote3=1., u_vote4=1., u_vote5=0.))\n\nwith MultiWorldCounterfactual() as mwc_v2:\nwith SearchForExplanation(antecedents = antecedents_v, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_v,\nconsequents = outcome_v,\nconsequent_scale= 1e-8):\nwith condition(data = observations_v2):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_v2:\nvoting_model()\n\nac_check(tr_v2, mwc_v2, antecedents_v, witnesses_v, outcome_v)\n\n\n# In[37]:\n\n\nantecedents_v3 = {\"vote0\":0.0, \"vote1\": 0.0}\nwitnesses_v3 = convert_to_bool([f\"vote{i}\" for i in range(2,6)])\n\nwith MultiWorldCounterfactual() as mwc_v3:\nwith SearchForExplanation(antecedents = antecedents_v3, antecedent_bias= antencedent_bias,\nwitnesses = witnesses_v3,\nconsequents = outcome_v,\nconsequent_scale= 1e-8):\nwith condition(data = observations_v2):\nwith pyro.plate(\"sample\", num_samples):\nwith pyro.poutine.trace() as tr_v3:\nvoting_model()\n\nac_check(tr_v3, mwc_v3, antecedents_v3, witnesses_v3, outcome_v)", "description": "Voting example using a model to explore the notion of actual causality and its sufficiency to capture intuitions about responsibility and blame."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/mediation.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport torch\nimport pytorch_lightning as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\npyro.clear_param_store()\npyro.set_rng_seed(1234)\npyro.settings.set(module_local_params=True)\n\nsmoke_test = ('CI' in os.environ)\nmax_epochs = 5 if smoke_test else 1500\nnum_samples = 10 if smoke_test else 1000\n\n# ## Setup\n\n# Let's start with loading all the dependencies we use in this example.\n\n# In[12]:\n\n\nget_ipython().run_line_magic('reload_ext', 'autoreload')\nget_ipython().run_line_magic('pdb', 'off')\n\nimport os\nimport torch\nimport pytorch_lightning as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport pyro\nimport pyro.distributions as dist\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\npyro.clear_param_store()\npyro.set_rng_seed(1234)\npyro.settings.set(module_local_params=True)\n\nsmoke_test = ('CI' in os.environ)\nmax_epochs = 5 if smoke_test else 1500\nnum_samples = 10 if smoke_test else 1000", "description": "Setting up the environment and loading necessary dependencies for the entire example."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/mediation.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pandas as pd\nimport torch\n\nDATA_URL = \"https://statsnotebook.io/blog/data_management/example_data/substance.csv\"\n\n# Let's load the data from the url\nDATA_URL = \"https://statsnotebook.io/blog/data_management/example_data/substance.csv\"\n\ndf = pd.read_csv(DATA_URL)\nprint(f\"Number of individuals: {len(df)}\")\ndf = df.dropna()  # for now, ignore rows with missing or invalid data\nnum_data = len(df)\nprint(f\"Number of individuals without missing values: {num_data}\")\n\ndata = {\n\"conflict\": torch.tensor(df[\"conflict\"].values, dtype=torch.float),\n\"gender\": torch.tensor(df[\"gender\"].values == \"Male\", dtype=torch.float),\n\"fam_int\": torch.tensor(df[\"fam_int\"].values, dtype=torch.float),\n\"dev_peer\": torch.tensor(df[\"dev_peer\"].values, dtype=torch.float),\n\"sub_exp\": torch.tensor(df[\"sub_exp\"].values, dtype=torch.float),\n\"sub_disorder\": torch.tensor(df[\"sub_disorder\"].values, dtype=torch.float),\n}\ncovariates = {\"conflict\": data[\"conflict\"], \"gender\": data[\"gender\"]}\n\n# Show the data", "description": "Loading and preprocessing the data from a URL."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/mediation.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom chirho.observational.handlers import condition\nclass MediationModel(pyro.nn.PyroModule):\n    def __init__(self):\n        super().__init__()\n\nDATA_URL = \"https://statsnotebook.io/blog/data_management/example_data/substance.csv\"\ndf = pd.read_csv(DATA_URL)\ndata = {\n\"conflict\": torch.tensor(df[\"conflict\"].values, dtype=torch.float),\n}\nclass MediationModel(pyro.nn.PyroModule):\ndef __init__(self):\nsuper().__init__()\nself.f_fam_int = torch.nn.Linear(2, 1)\nself.f_dev_peer = torch.nn.Linear(3, 1)\nself.f_sub_exp = torch.nn.Linear(3, 1)\nself.f_sub_disorder = torch.nn.Linear(4, 1)\nself.register_buffer(\"zero\", torch.tensor(0.))\nself.register_buffer(\"one\", torch.tensor(1.))\n\ndef forward(self) -> torch.Tensor:\ngender = pyro.sample(\"gender\", dist.Bernoulli(0.5 * self.one))\nconflict = pyro.sample(\"conflict\", dist.LogNormal(self.zero, self.one))\n\ncovariates = torch.cat(torch.broadcast_tensors(\nconflict[..., None], gender[..., None]\n), dim=-1)\n\nlogits_fam_int = self.f_fam_int(covariates)[..., 0]\nfam_int = pyro.sample(\"fam_int\", dist.Bernoulli(logits=logits_fam_int))\n\ncovariates_and_treatment = torch.cat(torch.broadcast_tensors(\nconflict[..., None], gender[..., None], fam_int[..., None]\n), dim=-1)\n\nlogits_dev_peer = self.f_dev_peer(covariates_and_treatment)[..., 0]\ndev_peer = pyro.sample(\"dev_peer\", dist.Bernoulli(logits=logits_dev_peer))\n\nlogits_sub_exp = self.f_sub_exp(covariates_and_treatment)[..., 0]\nsub_exp = pyro.sample(\"sub_exp\", dist.Bernoulli(logits=logits_sub_exp))\n\ncovariates_and_mediators = torch.cat(torch.broadcast_tensors(\nconflict[..., None], gender[..., None], dev_peer[..., None], sub_exp[..., None]\n), dim=-1)\n\nlogits_sub_disorder = self.f_sub_disorder(covariates_and_mediators)[..., 0]\nsub_disorder = pyro.sample(\"sub_disorder\", dist.Bernoulli(logits=logits_sub_disorder))\n\nreturn sub_disorder\n\n\n# Later we'll do maximum likelihood, so the code does not contain priors over the linear weights. Now we can instantiate the class and inspect the graph of the resulting model.\n\n# In[15]:\n\n", "description": "Defining the mediation analysis model with Pyro, including the model structure and parameters for future inference."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/mediation.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nfrom chirho.indexed.ops import gather, IndexSet\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.interventional.handlers import do\nclass MediationModel(pyro.nn.PyroModule):\n def __init__(self):\n   super().__init__()\n\nclass NaturalDirectEffectModel(pyro.nn.PyroModule):\n def __init__(self, causal_model: MediationModel):\n   super().__init__()\n\nclass NaturalDirectEffectModel(pyro.nn.PyroModule):\n\ndef __init__(self, causal_model: MediationModel):\nsuper().__init__()\nself.causal_model = causal_model\n\n@pyro.infer.config_enumerate\ndef forward(self, x, x_prime):\nwith MultiWorldCounterfactual(), \\\ndo(actions=dict(fam_int=(x, x_prime))), \\\ndo(actions=dict(sub_exp=lambda Z_: gather(Z_, IndexSet(fam_int={2})))), \\\npyro.plate(\"data\", size=x.shape[0], dim=-1):\n\nys = self.causal_model()\nys_xprime = gather(ys, IndexSet(fam_int={2}, sub_exp={0}))  # y_x'\nys_x = gather(ys, IndexSet(fam_int={1}, sub_exp={1}))  # y_x,z\nreturn ys_xprime - ys_x\n\n\n# We now apply this to the causal model we introduced, to obtain a new model representing the joint distribution over all factual and counterfactual variables necessary to estimate the ANDE (which are also sufficient for the estimation of ANIE):\n\n# In[17]:\n\n\nx0 = data[\"fam_int\"].new_full((num_data,), 0.)\nx1 = data[\"fam_int\"].new_full((num_data,), 1.)\n\nsurrogate_model = MediationModel()\nquery_model = NaturalDirectEffectModel(surrogate_model)\n", "description": "Defining the Natural Direct Effect Model to estimate the average natural direct effect with a focus on multiple interventions."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/mediation.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nimport pytorch_lightning as pl\nfrom chirho.observational.handlers import condition\nDATA_URL = \"https://statsnotebook.io/blog/data_management/example_data/substance.csv\"\ndf = pd.read_csv(DATA_URL)\ndata = {\n\"conflict\": torch.tensor(df[\"conflict\"].values, dtype=torch.float),\n}\nclass MediationModel(pyro.nn.PyroModule):\n def __init__(self):\n   super().__init__()\nclass ConditionedMediationModel(pyro.nn.PyroModule):\n def __init__(self, causal_model: MediationModel):\n   super().__init__()\n\nclass ConditionedMediationModel(pyro.nn.PyroModule):\ndef __init__(self, causal_model: MediationModel):\nsuper().__init__()\nself.causal_model = causal_model\n\ndef forward(self, data):\nwith condition(data=data), \\\npyro.plate(\"data\", size=num_data, dim=-1):\nreturn self.causal_model()\n\nconditioned_model = ConditionedMediationModel(surrogate_model)\npyro.render_model(conditioned_model, model_args=(data,))\n\n\n# Now we use the standard Pyro inferential tools (see [Pyro tutorials](https://pyro.ai/examples/) if this looks unclear). We implement maximum likelihood with stochastic variational inference and the Dirac-Delta variational approximation.\n\n# In[19]:\n\n\nclass LightningSVI(pl.LightningModule):\ndef __init__(self, elbo: pyro.infer.elbo.ELBOModule, **optim_params):\nsuper().__init__()\nself.optim_params = dict(optim_params)\nself.elbo = elbo\n\ndef configure_optimizers(self):\nreturn torch.optim.Adam(self.elbo.parameters(), **self.optim_params)\n\ndef training_step(self, batch, batch_idx):\nreturn self.elbo(dict(zip(sorted(data.keys()), batch)))\n\n\nguide = pyro.infer.autoguide.AutoDelta(conditioned_model)\nelbo = pyro.infer.Trace_ELBO()(conditioned_model, guide)\n\n# initialize\nelbo(data)\n\n# fit\ntrain_dataset = torch.utils.data.TensorDataset(*(v for k, v in sorted(data.items())))\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=num_data)\nsvi = LightningSVI(elbo, lr=0.03)\ntrainer = pl.Trainer(max_epochs=max_epochs, log_every_n_steps=1)", "description": "Conditioning the MediationModel on the data, setting up the inference environment to learn the model's parameters with maximum likelihood estimation using Pyro's SVI."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/mediation.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nfrom chirho.observational.handlers import condition\nDATA_URL = \"https://statsnotebook.io/blog/data_management/example_data/substance.csv\"\ndf = pd.read_csv(DATA_URL)\ndata = {\n\"conflict\": torch.tensor(df[\"conflict\"].values, dtype=torch.float),\n}\nclass NaturalDirectEffectModel(pyro.nn.PyroModule):\n def __init__(self, causal_model: MediationModel):\n   super().__init__()\n\nconditioned_query_model = condition(data=dict(fam_int=data[\"fam_int\"], **covariates))(query_model)\n\ndiscrete_posterior = pyro.infer.infer_discrete(first_available_dim=-8)(conditioned_query_model)\n\npredictive = pyro.infer.Predictive(discrete_posterior, guide=guide, num_samples=num_samples, return_sites=[\"_RETURN\"])\npredictive_samples = predictive(x0, x1)\n\nindividual_NDE_samples = predictive_samples[\"_RETURN\"]\n\n", "description": "Analyzing the natural direct effect by conditioning on data and using inferential tools in Pyro to execute the causal query."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/mediation.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport matplotlib.pyplot as plt\nindividual_NDE_samples = torch.tensor([0])\n\nindividual_NDE_mean = torch.mean(individual_NDE_samples.squeeze(), dim=0)\nNDE_samples = torch.mean(individual_NDE_samples.squeeze(), dim=-1)  # avg over datapoints\nNDE_mean = torch.mean(NDE_samples, dim=0)  # avg over posterior samples\nprint(NDE_mean)\n\n\n#  In the [original blog post](https://statsnotebook.io/blog/analysis/mediation/), the NDA was $-.055$ with confidence interval $(-.12, .01)$. We mark these with vertical lines. Our MLE falls within the original confidence interval.\n\n# In[22]:\n\n\nplt.hist(individual_NDE_mean.detach().cpu().numpy(), bins=25, range = (-.12, .02))\nplt.axvline(-.055, color='red', linestyle='solid', linewidth=1)\nplt.axvline(-.12, color='green', linestyle='dashed', linewidth=.5)\nplt.axvline(.01, color='green', linestyle='dashed', linewidth=.5)\nplt.xlabel('NDE')\nplt.ylabel('Count')", "description": "Evaluating the mean natural direct effect (NDE) from the model, along with creating a histogram visualization comparing the results to previous findings."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/dynamical_intro.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport warnings\nimport os\nimport matplotlib.pyplot as plt\nimport pyro\nimport pyro.distributions as dist\nimport seaborn as sns\nimport torch\nfrom pyro.infer import Predictive\nfrom pyro.infer.autoguide import AutoMultivariateNormal\nfrom chirho.dynamical.handlers import DynamicIntervention, LogTrajectory, StaticBatchObservation, StaticIntervention\nfrom chirho.dynamical.handlers.solver import TorchDiffEq\nfrom chirho.dynamical.ops import Dynamics, State, simulate\nfrom chirho.observational.handlers import condition\npyro.settings.set(module_local_params=True)\nsns.set_style(\"white\")\nseed = 123\npyro.clear_param_store()\npyro.set_rng_seed(seed)\nsmoke_test = ('CI' in os.environ)\nnum_steps = 10 if smoke_test else 1000\nnum_samples = 10 if smoke_test else 200\n\n# ## Setup\n#\n# Here, we install the necessary Pytorch, Pyro, and ChiRho dependencies for this example.\n\n# In[1]:\n\n\nget_ipython().run_line_magic('reload_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nimport os\nimport matplotlib.pyplot as plt\nimport pyro\nimport pyro.distributions as dist\nimport seaborn as sns\nimport torch\nfrom pyro.infer import Predictive\nfrom pyro.infer.autoguide import AutoMultivariateNormal\n\nfrom chirho.dynamical.handlers import (\nDynamicIntervention,\nLogTrajectory,\nStaticBatchObservation,\nStaticIntervention,\n)\nfrom chirho.dynamical.handlers.solver import TorchDiffEq\nfrom chirho.dynamical.ops import Dynamics, State, simulate\nfrom chirho.observational.handlers import condition\n\npyro.settings.set(module_local_params=True)\n\nsns.set_style(\"white\")\n\n# Set seed for reproducibility\nseed = 123\npyro.clear_param_store()\npyro.set_rng_seed(seed)\n\nsmoke_test = ('CI' in os.environ)\nnum_steps = 10 if smoke_test else 1000\nnum_samples = 10 if smoke_test else 200", "description": "Setup with necessary library imports and configurations for reproducibility."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/dynamical_intro.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pyro\nimport pyro.distributions as dist\nfrom chirho.dynamical.ops import State\nclass SIRDynamics(pyro.nn.PyroModule):\n    ...\ndef sir_observation_model(X: State[torch.Tensor]) -> None:\n    ...\n\n# In this section, we encode our causal assumptions about disease dynamics using an ordinary differential equation embedded in Pyro. To do this we'll first implement a `PyroModule` that describes the differential expressions declaratively, and then use a `ChiRho.dynamics.handlers.Solver` such as `TorchDiffEq` to solve the differential equation. Later in this example we'll extend the model with uncertainty.\n#\n# ### Model Description\n#\n# The `SIRDynamics` class encapsulates the dynamics of the SIR model. The model is defined by two key parameters: `beta` and `gamma`. These parameters govern the rate of infection and recovery, respectively. The `diff` method in the class defines the differential equations for the Susceptible (`S`), Infected (`I`), and Recovered (`R`) compartments. Specifically:\n#\n# - The rate of change of `S` is given by `-self.beta * X[\"S\"] * X[\"I\"]`, representing the transition of susceptible individuals to the infected state.\n#\n# - The rate of change of `I` is `self.beta * X[\"S\"] * X[\"I\"] - self.gamma * X[\"I\"]`, capturing both new infections and recoveries.\n#\n# - The rate of change of `R` is `self.gamma * X[\"I\"]`, representing the transition from infected to recovered.\n#\n# These equations encapsulate the causal relationships within the SIR model, where the number of susceptible and infected individuals causally influences the dynamics of the disease spread.\n#\n# ### Observation Model\n#\n# The `sir_observation_model` models how noisy data is generated from (latent) disease dynamics. In this model, we only observe the number of infected (`I_obs`) and recovered (`R_obs`) individuals. These observations are modeled as Poisson-distributed, capturing the inherent variability and uncertainty in real-world observations of infectious diseases.\n#\n\n# In[2]:\n\n\nclass SIRDynamics(pyro.nn.PyroModule):\ndef __init__(self, beta, gamma):\nsuper().__init__()\nself.beta = beta\nself.gamma = gamma\n\ndef forward(self, X: State[torch.Tensor]):\ndX = dict()\ndX[\"S\"] = -self.beta * X[\"S\"] * X[\"I\"]\ndX[\"I\"] = self.beta * X[\"S\"] * X[\"I\"] - self.gamma * X[\"I\"]\ndX[\"R\"] = self.gamma * X[\"I\"]\nreturn dX\n\ndef sir_observation_model(X: State[torch.Tensor]) -> None:\n# We don't observe the number of susceptible individuals directly.\n\n# Note: Here we set the event_dim to 1 if the last dimension of X[\"I\"] is > 1, as the sir_observation_model\n# can be used for both single and multi-dimensional observations.\nevent_dim = 1 if X[\"I\"].shape and X[\"I\"].shape[-1] > 1 else 0\npyro.sample(\"I_obs\", dist.Poisson(X[\"I\"]).to_event(event_dim))  # noisy number of infected actually observed\npyro.sample(\"R_obs\", dist.Poisson(X[\"R\"]).to_event(event_dim))  # noisy number of recovered actually observed", "description": "Encoding causal assumptions using an ordinary differential equation implemented in Pyro, capturing the SIR model dynamics and observation model."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/dynamical_intro.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nfrom chirho.dynamical.handlers.solver import TorchDiffEq\nfrom chirho.dynamical.ops import simulate, State\nfrom pyro.infer import Predictive\nfrom chirho.dynamical.handlers import LogTrajectory\nclass SIRDynamics(pyro.nn.PyroModule):\n    ...\ndef sir_observation_model(X: State[torch.Tensor]) -> None:\n    ...\n\n# ### Generating Synthetic Disease Data using `simulate`\n#\n# Using our `SIRDynamics` model, we can generate synthetic data both for the (latent) true dynamics, as well as the noisy observations. In this scenario\n# we'll assume that the epidemic began at $t=0$, we gather measurements from $t=0.5$ to $t=1$ months, and that we are interesting in forecasting disease dynamics until $t=3$ months.\n#\n# For this scenario we'll assume that the true infection rate is $\\beta=0.03$ and the true recovery rate is $\\gamma=0.5$.\n#\n# **Note:** Here we use a new effectful operation introduced in the `chirho.dynamical` module, `simulate`, which solves the differential equation passed as the `dynamics` argument. Similar to `pyro.sample`, `simulate`'s behavior can be modified using several effect handlers. In the following code snippet we show a `Solver` handler, `TorchDiffEq`, which wraps the torchdiffeq.py (https://github.com/rtqichen/torchdiffeq) package for solving ODEs for use with `chirho`. In addition, we use the `LogTrajectory` handler to store the results of the simulation at multiple points in time, rather than the default behavior of just returning the terminal state.\n\n# In[3]:\n\n\n# Assume there is initially a population of 99 million people that are susceptible, 1 million infected, and 0 recovered\ninit_state = dict(S=torch.tensor(99.0), I=torch.tensor(1.0), R=torch.tensor(0.0))\nstart_time = torch.tensor(0.0)\nend_time = torch.tensor(3.0)\nstep_size = torch.tensor(0.1)\nlogging_times = torch.arange(start_time, end_time, step_size)\n\n# We now simulate from the SIR model\nbeta_true = torch.tensor(0.03)\ngamma_true = torch.tensor(0.5)\nsir_true = SIRDynamics(beta_true, gamma_true)\nwith TorchDiffEq(), LogTrajectory(logging_times) as lt:\nsimulate(sir_true, init_state, start_time, end_time)\n\nsir_true_traj = lt.trajectory\n\nobs_start_time = torch.tensor(0.5) # Measurements start 0.5 month into the pandemic\nobs_sample_rate = torch.tensor(1/30) # Take measurements one per day\nobs_end_time = torch.tensor(1.0) # Measurements end after 1st month\n\nobs_logging_times = torch.arange(obs_start_time, obs_end_time, obs_sample_rate)\nN_obs = obs_logging_times.shape[0]\nwith TorchDiffEq(), LogTrajectory(obs_logging_times) as lt_obs:\nsimulate(sir_true, init_state, start_time, obs_end_time)\n\nsir_obs_traj = lt_obs.trajectory\nwith pyro.poutine.trace() as tr:\nsir_observation_model(sir_obs_traj)\n", "description": "Generating synthetic disease data using the defined `SIRDynamics` model. This includes solving the SIR model to generate the disease trajectory and observation data."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/dynamical_intro.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pyro\nimport pyro.distributions as dist\nfrom chirho.dynamical.ops import Dynamics\n\ndef bayesian_sir(base_model=SIRDynamics) -> Dynamics[torch.Tensor]:\n    ...\n\n# ### Extending the `SIRDynamics` model with uncertainty over parameters\n#\n# In our [tutorial](tutorial_i.ipynb) we extended our deterministic causal model to include uncertainty by adding prior distributions on model parameters. Even though our model here takes the form of a differential equation, it can still be extended to include uncertainty succinctly in exactly the same way. In the following code blocks we add uniform priors over $\\beta$ and $\\gamma$.\n\n# In[5]:\n\n\n# We place uniform priors on the beta and gamma parameters defining the SIR model\ndef bayesian_sir(base_model=SIRDynamics) -> Dynamics[torch.Tensor]:\nbeta = pyro.sample(\"beta\", dist.Uniform(0, 1))\ngamma = pyro.sample(\"gamma\", dist.Uniform(0, 1))\nsir = base_model(beta, gamma)\nreturn sir", "description": "Extending the `SIRDynamics` model with uncertainty by adding uniform priors over its parameters `beta` and `gamma`."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/dynamical_intro.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pyro.infer import Predictive\n\n\ndef simulated_bayesian_sir(init_state, start_time, logging_times, base_model=SIRDynamics) -> State[torch.Tensor]:\n    ...\n\ndef SIR_plot(time_period, state_pred, data, ylabel, color, data_label, ax, legend=False, test_plot=True, test_start_time=None, test_end_time=None, mean_label=\"Posterior Mean\"):\n    ...\n\n# ### Informal Prior Predictive Check - Visualizing Samples\n#\n# To see how our uncertainty over parameters propagates to uncertainty over disease trajectories, we can visualize samples from the prior predictive distribution.\n\n# In[6]:\n\n\nprior_predictive = Predictive(simulated_bayesian_sir, num_samples=num_samples)\nsir_prior_samples = prior_predictive(init_state, start_time, logging_times)\n\n\n# In[7]:\n\n\ndef SIR_uncertainty_plot(time_period, state_pred, ylabel, color, ax, mean_label=\"Posterior Mean\"):\nsns.lineplot(\nx=time_period,\ny=state_pred.mean(dim=0),\ncolor=color,\nlabel=mean_label,\nax=ax,\n)\n# 95% Credible Interval\nax.fill_between(\ntime_period,\ntorch.quantile(state_pred, 0.025, dim=0),\ntorch.quantile(state_pred, 0.975, dim=0),\nalpha=0.2,\ncolor=color,\nlabel=\"95% Credible Interval\",\n)\n\nax.set_xlabel(\"Time (Months)\")\nax.set_ylabel(ylabel)\n\n\ndef SIR_data_plot(time_period, data, data_label, ax):\nsns.lineplot(\nx=time_period, y=data, color=\"black\", ax=ax, linestyle=\"--\", label=data_label\n)\n\n\ndef SIR_test_plot(test_start_time, test_end_time, ax):\nax.axvline(\ntest_start_time, color=\"black\", linestyle=\":\", label=\"Measurement Period\"\n)\nax.axvline(\ntest_end_time, color=\"black\", linestyle=\":\"\n)\n\n\ndef SIR_plot(\ntime_period,\nstate_pred,\ndata,\nylabel,\ncolor,\ndata_label,\nax,\nlegend=False,\ntest_plot=True,\ntest_start_time=obs_start_time,\ntest_end_time=obs_end_time,\nmean_label=\"Posterior Mean\",\n):\nSIR_uncertainty_plot(time_period, state_pred, ylabel, color, ax, mean_label=mean_label)\nSIR_data_plot(time_period, data, data_label, ax)\nif test_plot:\nSIR_test_plot(test_start_time, test_end_time, ax)\nif legend:\nax.legend()\nelse:\nax.legend().remove()\nsns.despine()\n\n\n# In[8]:\n\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nSIR_plot(\nlogging_times,\nsir_prior_samples[\"S\"],\nsir_true_traj[\"S\"],\n\"# Susceptible (Millions)\",\n\"orange\",\n\"Ground Truth\",\nax[0],\nlegend=True,\ntest_plot=False,\nmean_label=\"Prior Mean\",\n)\n\nSIR_plot(\nlogging_times,\nsir_prior_samples[\"I\"],\nsir_true_traj[\"I\"],\n\"# Infected (Millions)\",\n\"red\",\n\"Ground Truth\",\nax[1],\nlegend=True,\ntest_plot=False,\nmean_label=\"Prior Mean\",\n)\n\nSIR_plot(\nlogging_times,\nsir_prior_samples[\"R\"],\nsir_true_traj[\"R\"],\n\"# Recovered (Millions)\",\n\"green\",\n\"Ground Truth\",\nax[2],\nlegend=True,\ntest_plot=False,\nmean_label=\"Prior Mean\",", "description": "Conducting an informal prior predictive check by visualizing samples from the prior predictive distribution to see how uncertainty over parameters propagates to disease trajectories."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/dynamical_intro.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pyro\nfrom pyro.infer import Trace_ELBO\nfrom pyro.infer.autoguide import AutoMultivariateNormal\nimport torch\n\ndef conditioned_sir(obs_times, data, init_state, start_time, base_model=SIRDynamics) -> None:\n    ...\ndef run_svi_inference(model, num_steps, verbose=True, lr=.03, vi_family=AutoMultivariateNormal, guide=None, **model_kwargs):\n    ...\n\n# ## Probabilistic Inference over Dynamical System Parameters\n#\n# One of the major benefits of writing our dynamical systems model in Pyro and ChiRho is that we can leverage Pyro's support for (partially) automated probabilistic inference. In this section we'll (i) condition on observational data using the `StaticBatchObservation` effect handler and (ii) optimize a variational approximation to the posterior using Pyro's SVI utilities.\n\n# In[9]:\n\n\ndef conditioned_sir(obs_times, data, init_state, start_time, base_model=SIRDynamics) -> None:\nsir = bayesian_sir(base_model)\nobs = condition(data=data)(sir_observation_model)\nwith TorchDiffEq(), StaticBatchObservation(obs_times, observation=obs):\nsimulate(sir, init_state, start_time, obs_times[-1])\n\n# Define a helper function to run SVI. (Generally, Pyro users like to have more control over the training process!)\ndef run_svi_inference(model, num_steps=num_steps, verbose=True, lr=.03, vi_family=AutoMultivariateNormal, guide=None, **model_kwargs):\nif guide is None:\nguide = vi_family(model)\nelbo = pyro.infer.Trace_ELBO()(model, guide)\n# initialize parameters\nelbo(**model_kwargs)\nadam = torch.optim.Adam(elbo.parameters(), lr=lr)\n# Do gradient steps\nfor step in range(1, num_steps + 1):\nadam.zero_grad()\nloss = elbo(**model_kwargs)\nloss.backward()\nadam.step()\nif (step % 100 == 0) or (step == 1) & verbose:\nprint(\"[iteration %04d] loss: %.4f\" % (step, loss))\nreturn guide\n\n\n# In[10]:\n\n\n# Run inference to approximate the posterior distribution of the SIR model parameters\nsir_guide = run_svi_inference(\nconditioned_sir,\nnum_steps=num_steps,\nobs_times=obs_logging_times,\ndata=sir_data,\ninit_state=init_state,\nstart_time=start_time,", "description": "Demonstrating probabilistic inference over dynamical system parameters using Pyro, including conditioning on observational data and optimizing a variational approximation to the posterior."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/dynamical_intro.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pyro\nimport torch\nfrom chirho.dynamical.ops import State\nfrom pyro.infer import Predictive\nfrom chirho.dynamical.handlers import DynamicIntervention, StaticIntervention\nfrom chirho.dynamical.ops import simulate\n\n\nclass SIRDynamicsLockdown(SIRDynamics):\n    ...\n\ndef intervened_sir(lockdown_start, lockdown_end, lockdown_strength, init_state, start_time, logging_times) -> State[torch.Tensor]:\n    ...\n\ndef dynamic_intervened_sir(lockdown_trigger, lockdown_lift_trigger, lockdown_strength, init_state, start_time, logging_times) -> State:\n    ...\n\n# ## Exploring Interventions\n\n# Suppose the government can enact different lockdown measures (of varying strength) to flatten the infection curve. Following [2], we define the stength of lockdown measure at time $t$ by $l(t) \\in [0, 1]$ for $1 \\leq t \\leq T$. Parametrize the transmission rate $\\beta_t$ as:\n#\n# $$\n# \\beta(t) = (1 - l(t)) \\beta_0,\n# $$\n#\n# where $\\beta_0$ denotes the unmitigated transmission rate and larger values of $l(t)$ correspond to stronger lockdown measures. Then, the time-varying SIR model is defined as follows:\n#\n# $$\n# \\begin{split}\n#     dS/dt &= -\\beta(t) S I \\\\\n#     dI/dt &= \\beta(t) S I - \\gamma I \\\\\n#     dR/dt &= \\gamma I\n# \\end{split}\n# $$\n#\n# where $S, I, R$ denote the number of susceptible, infected, and recovered individuals at time $t$ for $1 \\leq t \\leq T$.\n#\n# We can implement this new model compositionally using our existing SIR model implementation.\n\n# In[14]:\n\n\nclass SIRDynamicsLockdown(SIRDynamics):\ndef __init__(self, beta0, gamma):\nsuper().__init__(beta0, gamma)\nself.beta0 = beta0\n\ndef forward(self, X: State[torch.Tensor]):\nself.beta = (1 - X[\"l\"]) * self.beta0  # time-varing beta parametrized by lockdown strength l_t\ndX = super().forward(X)\ndX[\"l\"] = torch.tensor(0.0)\nreturn dX\n\n\ninit_state_lockdown = dict(**init_state, l=torch.tensor(0.0))\n\n\n# ### Modeling a Deterministic Intervention\n#\n# Let's first look at a deterministic intervention where the transmission rate is reduced by 75% between $t=1$ and $t=2$ due to stronger lockdown measures.\n#\n# To implement this succinctly we'll use ChiRho's `StaticIntervention` handlers, which interrupt the enclosing `simulate` call to change the value of the `State` at a particular moment in time. These handlers compose freely with each other, and with the `LogTrajectory` and `TorchDiffEq` introduces earlier.\n\n# In[15]:\n\n\ndef intervened_sir(lockdown_start, lockdown_end, lockdown_strength, init_state, start_time, logging_times) -> State[torch.Tensor]:\nsir = bayesian_sir(SIRDynamicsLockdown)\nwith LogTrajectory(logging_times, is_traced=True) as lt:\nwith TorchDiffEq():\nwith StaticIntervention(time=lockdown_start, intervention=dict(l=lockdown_strength)):\nwith StaticIntervention(time=lockdown_end, intervention=dict(l=torch.tensor(0.0))):\nsimulate(sir, init_state, start_time, logging_times[-1])\n\nreturn lt.trajectory\n\n\n# We see in the figure below that this lockdown measures indeed \"flattens\" the curve.\n\n# In[16]:\n\n\nlockdown_start = torch.tensor(1.0)\nlockdown_end = torch.tensor(2.0)\nlockdown_strength = torch.tensor(0.75)\n\ntrue_intervened_sir = pyro.condition(intervened_sir, data={\"beta\": beta_true, \"gamma\": gamma_true})\ntrue_intervened_trajectory = true_intervened_sir(lockdown_start, lockdown_end, lockdown_strength, init_state_lockdown, start_time, logging_times)\n\nintervened_sir_predictive = Predictive(intervened_sir, guide=sir_guide, num_samples=num_samples)\nintervened_sir_posterior_samples = intervened_sir_predictive(lockdown_start, lockdown_end, lockdown_strength, init_state_lockdown, start_time, logging_times)\n\n\n# In[17]:\n\n\n# Plot predicted values for S, I, and R with 95% credible intervals\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nSIR_plot(\nlogging_times,\nintervened_sir_posterior_samples[\"S\"],\ntrue_intervened_trajectory[\"S\"],\n\"# Susceptible (Millions)\",\n\"orange\",\n\"Ground Truth\",\nax[0],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\nintervened_sir_posterior_samples[\"I\"],\ntrue_intervened_trajectory[\"I\"],\n\"# Infected (Millions)\",\n\"red\",\n\"Ground Truth\",\nax[1],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\nintervened_sir_posterior_samples[\"R\"],\ntrue_intervened_trajectory[\"R\"],\n\"# Recovered (Millions)\",\n\"green\",\n\"Ground Truth\",\nax[2],\nlegend=True,\ntest_plot=False,\n)\n\n# Plot the static intervention\nfor a in ax:\na.axvline(lockdown_start, color=\"grey\", linestyle=\"-\", label=\"Start of Lockdown\")\na.axvline(lockdown_end, color=\"grey\", linestyle=\"-\", label=\"End of Lockdown\")\na.legend()\n\n\n# ### Modeling an Uncertain Intervention\n#\n# In the previous example we assumed that the time the intervention was applied was entirely determined by the policymakers. However, in practice, how an intervention is implemented may depend on many external factors that out of the policymaker's control. Instead, we'd like to represent additional uncertainty over when the intervention is applied. It turns out that implementing this extension is remarkably straightforward using ChiRho, we simply call the `intervened_sir` model with `lockdown_start` and `lockdown_end` drawn from some distribution as follows:\n\n# In[18]:\n\n\nlockdown_start_min = torch.tensor(0.5)\nlockdown_start_max = torch.tensor(1.5)\n\nlockdown_end_min = torch.tensor(2.0)\nlockdown_end_max = torch.tensor(2.5)\n\ndef uncertain_intervened_sir(lockdown_strength, init_state, start_time, logging_times) -> State:\nlockdown_start = pyro.sample(\"lockdown_start\", dist.Uniform(lockdown_start_min, lockdown_start_max))\nlockdown_end = pyro.sample(\"lockdown_end\", dist.Uniform(lockdown_start_min, lockdown_start_max))\nreturn intervened_sir(lockdown_start, lockdown_end, lockdown_strength, init_state, start_time, logging_times)\n\n\n# In[19]:\n\n\nuncertain_intervened_sir_predictive = Predictive(uncertain_intervened_sir, guide=sir_guide, num_samples=num_samples)\nuncertain_intervened_sir_posterior_samples = uncertain_intervened_sir_predictive(lockdown_strength, init_state_lockdown, start_time, logging_times)\n\n\n# In[20]:\n\n\n# Plot predicted values for S, I, and R with 95% credible intervals\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nSIR_plot(\nlogging_times,\nuncertain_intervened_sir_posterior_samples[\"S\"],\ntrue_intervened_trajectory[\"S\"],\n\"# Susceptible (Millions)\",\n\"orange\",\n\"Ground Truth\",\nax[0],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\nuncertain_intervened_sir_posterior_samples[\"I\"],\ntrue_intervened_trajectory[\"I\"],\n\"# Infected (Millions)\",\n\"red\",\n\"Ground Truth\",\nax[1],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\nuncertain_intervened_sir_posterior_samples[\"R\"],\ntrue_intervened_trajectory[\"R\"],\n\"# Recovered (Millions)\",\n\"green\",\n\"Ground Truth\",\nax[2],\nlegend=True,\ntest_plot=False,\n)\n\n# Plot the static intervention\nfor a in ax:\na.axvspan(lockdown_start_min, lockdown_start_max, color=\"grey\", linestyle=\"-\", label=\"Start of Lockdown\", alpha=0.15)\na.axvspan(lockdown_end_min, lockdown_end_max, color=\"grey\", linestyle=\"-\", label=\"End of Lockdown\", alpha=0.15)\na.legend()\n\n\n# ### Modeling a State-Dependent Intervention\n#\n# In the previous example we assumed that the intervention was applied at some (uncertain) moment in time, but was independent of the disease dynamics themselves. However, more practical interventions may be expressed as policies that dynamically respond to the dynamical system's state.\n#\n# Using our running example, let's assume that the government will issue a lockdown measure that reduces the transmission rate by 90% whenever the number of infected people hits 30 million infected. The government removes this lockdown when 20% of the population is recovered. Importantly, here we don't know a priori when this event will happen, instead we need the intervention to be applied dynamically based on the results of the simulation as it is run.\n#\n# To implement this we'll use ChiRho's `DynamicIntervention` handler, which takes as input an `event_fn`, describing when to apply the intervention, and an `intervention`, describing what happens when the intervention is applied.\n#\n# **Note:** The `DynamicIntervention` is strictly more expressive than the `StaticIntervention` described before, as one can define an `event_fn` that only triggers when $t$ is above a specific value. However, this expressiveness comes with additional runtime costs, as the solver must now solve a root-finding problem during the simulation. Therefore, we recommend using the `StaticIntervention` when the intervention time is independent of the state, and a `DynamicIntervention` otherwise.\n\n# In[21]:\n\n\ndef government_lockdown_policy(target_state: State[torch.tensor]):\n# Note: The event function must be a function of positional arguments t and state as follows:\n# In this case we are only interested in the number of infected individuals, so the body of the event function\n# is simply the difference between the number of infected individuals and the target number of infected individuals.\ndef event_f(t: torch.tensor, state: State[torch.tensor]):\nreturn state[\"I\"] - target_state[\"I\"]\n\nreturn event_f\n\n\ndef government_lift_policy(target_state: State[torch.tensor]):\n# Note: See above comment for `government_lockdown_policy`.\ndef event_f(t: torch.tensor, state: State[torch.tensor]):\nreturn target_state[\"R\"] - state[\"R\"]\n\nreturn event_f\n\n\ndef dynamic_intervened_sir(lockdown_trigger, lockdown_lift_trigger, lockdown_strength, init_state, start_time, logging_times) -> State:\nsir = bayesian_sir(SIRDynamicsLockdown)\nwith LogTrajectory(logging_times, is_traced=True) as lt:\nwith TorchDiffEq():\nwith DynamicIntervention(event_fn=government_lockdown_policy(lockdown_trigger), intervention=dict(l=lockdown_strength)):\nwith DynamicIntervention(event_fn=government_lift_policy(lockdown_lift_trigger), intervention=dict(l=torch.tensor(0.0))):\nsimulate(sir, init_state, start_time, logging_times[-1])\nreturn lt.trajectory\n\n\n# In[22]:\n\n\nlockdown_trigger = dict(I=torch.tensor(30.0))\nlockdown_lift_trigger = dict(R=torch.tensor(20.0))\nlockdown_strength = torch.tensor(0.9)  # reduces transmission rate by 90%\n\ntrue_dynamic_intervened_sir = pyro.condition(dynamic_intervened_sir, data={\"beta\": beta_true, \"gamma\": gamma_true})\ntrue_dynamic_intervened_trajectory = true_dynamic_intervened_sir(lockdown_trigger, lockdown_lift_trigger, lockdown_strength, init_state_lockdown, start_time, logging_times)\n\ndynamic_intervened_sir_predictive = Predictive(dynamic_intervened_sir, guide=sir_guide, num_samples=num_samples)\ndynamic_intervened_sir_posterior_samples = dynamic_intervened_sir_predictive(lockdown_trigger, lockdown_lift_trigger, lockdown_strength, init_state_lockdown, start_time, logging_times)\n\n\n# In[23]:\n\n\n# Plot predicted values for S, I, and R with 95% credible intervals\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nSIR_plot(\nlogging_times,\ndynamic_intervened_sir_posterior_samples[\"S\"],\ntrue_dynamic_intervened_trajectory[\"S\"],\n\"# Susceptible (Millions)\",\n\"orange\",\n\"Ground Truth\",\nax[0],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\ndynamic_intervened_sir_posterior_samples[\"I\"],\ntrue_dynamic_intervened_trajectory[\"I\"],\n\"# Infected (Millions)\",\n\"red\",\n\"Ground Truth\",\nax[1],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\ndynamic_intervened_sir_posterior_samples[\"R\"],\ntrue_dynamic_intervened_trajectory[\"R\"],\n\"# Recovered (Millions)\",\n\"green\",\n\"Ground Truth\",\nax[2],\nlegend=True,\ntest_plot=False,\n)\n\n# Draw horizontal line at lockdown trigger\nax[1].axhline(lockdown_trigger[\"I\"], color=\"grey\", linestyle=\"-\", label=\"Lockdown Trigger\")\nax[1].legend()\nax[2].axhline(lockdown_lift_trigger[\"R\"], color=\"grey\", linestyle=\"-\", label=\"Lockdown Lift Trigger\")\nax[2].legend()\n\n\n# ### Modeling a State-Dependent Intervention with Uncertainty\n#\n# Perhaps not surprisingly, we can also extend our `dynamic_intervened_sir` model to include uncertainty about the `lockdown_trigger` and `lockdown_lift` conditions themselves. Just as before, this can be accomplished simply by calling the `dynamic_intervened_sir` model with arguments drawn from some distribution as follows:\n\n# In[24]:\n\n\nlockdown_trigger_min = torch.tensor(20.0)\nlockdown_trigger_max = torch.tensor(40.0)\nlockdown_lift_trigger_min = torch.tensor(10.0)\nlockdown_lift_trigger_max = torch.tensor(30.0)\n\n\ndef uncertain_dynamic_intervened_sir(lockdown_strength, init_state, start_time, logging_times) -> State:\nlockdown_trigger = dict(I=pyro.sample(\"lockdown_trigger\", dist.Uniform(lockdown_trigger_min, lockdown_trigger_max)))\nlockdown_lift_trigger = dict(R=pyro.sample(\"lockdown_lift_trigger\", dist.Uniform(lockdown_lift_trigger_min, lockdown_lift_trigger_max)))\nreturn dynamic_intervened_sir(lockdown_trigger, lockdown_lift_trigger, lockdown_strength, init_state, start_time, logging_times)\n\n\n# In[25]:\n\n\nuncertain_dynamic_intervened_sir_predictive = Predictive(uncertain_dynamic_intervened_sir, guide=sir_guide, num_samples=num_samples)\nuncertain_dynamic_intervened_sir_posterior_samples = (uncertain_dynamic_intervened_sir_predictive(lockdown_strength, init_state_lockdown, start_time, logging_times))\n\n\n# In[26]:\n\n\n# Plot predicted values for S, I, and R with 95% credible intervals\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\nSIR_plot(\nlogging_times,\nuncertain_dynamic_intervened_sir_posterior_samples[\"S\"],\ntrue_dynamic_intervened_trajectory[\"S\"],\n\"# Susceptible (Millions)\",\n\"orange\",\n\"Ground Truth\",\nax[0],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\nuncertain_dynamic_intervened_sir_posterior_samples[\"I\"],\ntrue_dynamic_intervened_trajectory[\"I\"],\n\"# Infected (Millions)\",\n\"red\",\n\"Ground Truth\",\nax[1],\nlegend=True,\ntest_plot=False,\n)\nSIR_plot(\nlogging_times,\nuncertain_dynamic_intervened_sir_posterior_samples[\"R\"],\ntrue_dynamic_intervened_trajectory[\"R\"],\n\"# Recovered (Millions)\",\n\"green\",\n\"Ground Truth\",\nax[2],\nlegend=True,\ntest_plot=False,\n)\n\n# Draw horizontal line at lockdown trigger\nax[1].axhspan(lockdown_trigger_min, lockdown_trigger_max, color=\"grey\", linestyle=\"-\", label=\"Lockdown Trigger\", alpha=0.15)\nax[1].legend()\nax[2].axhspan(lockdown_lift_trigger_min, lockdown_lift_trigger_max, color=\"grey\", linestyle=\"-\", label=\"Lockdown Lift Trigger\", alpha=0.15)", "description": "Exploring the impact of different interventions on the SIR model dynamics through several scenarios, including deterministic and uncertain interventions, and state-dependent policies with and without uncertainty."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\n\n# ## Dev setup\n#\n# Here, we install the necessary Pytorch, Pyro, and ChiRho dependencies for this example.\n\n# In[1]:\n\n\nget_ipython().run_line_magic('load_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\n\n\n# In[2]:\n\n\nget_ipython().run_cell_magic('capture', '', '!pip install scanpy pybiomart numpy==1.23 seaborn==0.12.2 # add single cell dependencies\\n')\n\n\n# In[3]:\n\n\nimport os\nimport math\nimport numpy as np\nimport scipy.sparse\nimport pandas as pd\nimport scanpy as sc\nfrom scipy.stats import pearsonr\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyro\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.nn import PyroModule\nimport pyro.distributions as dist\nfrom IPython.display import Image\nimport torch\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\npyro.clear_param_store()\npyro.set_rng_seed(1234)\npyro.settings.set(module_local_params=True)\n", "description": "This example demonstrates setting up the development environment for the Chirho library by installing necessary dependencies like PyTorch, Pyro, and ChiRho itself. It includes the cells responsible for environment setup, including package installations and imports, as well as the initialization settings for Pyro and Seaborn for plotting."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport os\nfrom IPython.display import Image\n\n# ## Understanding the mechanism of action of a drug using causal inference\n\n# ### Experimental design\n#\n# We analyze the SciPlex3 dataset from Srivatsan (2020), which uses \"nuclear hashing\" to measure global transcriptional responses to over 4,000 unique perturbations at single-cell resolution. This hashing technique is summarized below:\n\n# In[4]:\n\n\n# Figure 1B from Srivatsan (2020)\n# Cells corresponding to different perturbations are lysed in-well, their nuclei labeled with well-specific \u201chash\u201d oligos,\n# followed by fixation, pooling, and sci-RNA-seq.\nImage('./figures/sciplex_fig1b.png', width=500)\n\n\n# Using this experimental setup, Srivatsan (2020) profiled the effects of 188 drugs on three different cancer cell types: MCF7, K562, and A549. Each cell in this dataset is unperturbed (`Vehicle`) or randomly treated with a drug from a library of 188 drug perburbations (`product_name`) at 4 different dosages (`dose`): $10^1, 10^2, 10^3, 10^4$ nM. After 24hrs, the expressions of each gene are measured. Hence, apart from technical confounders such as batch effects, the differences in gene expression between the treated and untreated cells are caused by the drug perturbation. Figure 3A summarizes this combinatorial experimental design.\n\n# In[5]:\n\n", "description": "Illustrates the concept of experimental design using the SciPlex3 dataset. It presents an overview of how the dataset measures transcriptional responses to drug perturbations and includes code for displaying relevant figures from cited research papers."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport pandas as pd\nimport scipy.sparse\nimport scanpy as sc\n\n# ### Data ingestion\n#\n# scPerturb (https://github.com/sanderlab/scPerturb/) provides pre-processed single-cell RNA seq datasets, including the SciPlex3 dataset from Srivatsan (2020). Unfortunately, there is an issue in scPerturb with processing the SciPlex3 dataset (https://github.com/sanderlab/scPerturb/issues/7). We fix the bug in their script (https://github.com/sanderlab/scPerturb/blob/master/dataset_processing/SrivatsanTrapnell2020.py) below.\n\n# In[6]:\n\n\n# Download raw data if doesn't already exist\nif os.path.exists(\"SrivatsanTrapnell2020_sciplex3_debugged.h5\"):\nanndata = sc.read_h5ad(\"SrivatsanTrapnell2020_sciplex3_debugged.h5\")\nelse:\n# Download and unzip 3 files files from here:\n# https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM4150378\n# (1) GSM4150378_sciPlex3_A549_MCF7_K562_screen_UMI.count.matrix.gz\n# (2) GSM4150378_sciPlex3_A549_MCF7_K562_screen_gene.annotations.txt.gz\n# (3) GSM4150378_sciPlex3_pData.txt.gz\n\n# Cach raw zip files\nos.system(\n\"wget -c https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM4150nnn/GSM4150378/suppl/GSM4150378_sciPlex3_A549_MCF7_K562_screen_UMI.count.matrix.gz\"\n)\nos.system(\n\"wget -c https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM4150nnn/GSM4150378/suppl/GSM4150378_sciPlex3_A549_MCF7_K562_screen_gene.annotations.txt.gz\"\n)\n\nos.system(\n\"wget -c https://ftp.ncbi.nlm.nih.gov/geo/samples/GSM4150nnn/GSM4150378/suppl/GSM4150378_sciPlex3_pData.txt.gz\"\n)\n\n# unzip files\nos.system(\"gunzip GSM4150378_sciPlex3_A549_MCF7_K562_screen_UMI.count.matrix.gz\")\nos.system(\"gunzip GSM4150378_sciPlex3_A549_MCF7_K562_screen_gene.annotations.txt.gz\")\nos.system(\"gunzip GSM4150378_sciPlex3_pData.txt.gz\")\n\nvar = pd.read_csv(\n\"GSM4150378_sciPlex3_A549_MCF7_K562_screen_gene.annotations.txt\",\nsep=\" \",\n)\nvar.columns = [\"gene_id\", \"gene_name\"]\n\nvar = var.set_index(\"gene_name\")\n\nobs2 = pd.read_csv(\"GSM4150378_sciPlex3_pData.txt\", sep=\" \")\n\nUMI_counts = pd.read_csv(\n\"GSM4150378_sciPlex3_A549_MCF7_K562_screen_UMI.count.matrix\", sep=\"\\t\", header=None\n)\n\nX = scipy.sparse.csr_matrix(\n(UMI_counts[2], (UMI_counts[1] - 1, UMI_counts[0] - 1)), shape=(len(obs2), len(var))\n)  # this may crash your kernel\n\nUMI_counts = None  # save on memory\nanndata = sc.AnnData(X, obs=obs2, var=var)\n\n# reform var\nanndata.var.index = anndata.var.index.rename(\"gene_symbol\")\nanndata.var[\"ensembl_gene_id\"] = [x.split(\".\")[0] for x in anndata.var[\"gene_id\"]]\nanndata.obs[\"dose_unit\"] = \"nM\"\nanndata.obs[\"celltype\"] = [\n\"alveolar basal epithelial cells\"\nif line == \"A549\"\nelse \"mammary epithelial cells\"\nif line == \"MCF7\"\nelse \"lymphoblasts\"\nif line == \"K562\"\nelse \"None\"\nfor line in anndata.obs.cell_type\n]\nanndata.obs[\"disease\"] = [\n\"lung adenocarcinoma\"\nif line == \"A549\"\nelse \"breast adenocarcinoma\"\nif line == \"MCF7\"\nelse \"chronic myelogenous leukemia\"\nif line == \"K562\"\nelse \"None\"\nfor line in anndata.obs.cell_type\n]\nanndata.write(\"SrivatsanTrapnell2020_sciplex3_debugged.h5\")\n\n\n# In[7]:\n\n", "description": "Covers the data ingestion process where the SciPlex3 dataset is downloaded, processed, and loaded into an Anndata object. It details the steps to correct a bug in scPerturb's processing script, download the raw datasets, unzip them, and finally load them into a format suitable for analysis."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pandas as pd\nimport scipy.sparse\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scanpy as sc\n\n# ### Data preprocessing\n#\n# We preprocess the data as follows:\n# - Filter out cells that have unknown cell type\n# - Only include measurements at the 24hr timepoint (retains ~90% of the dataset)\n# - Restrict to human genes since all cells are from human cell lines\n# - Filter out genes that are expressed in fewer than 10 cells\n# - Filter out cells that have fewer than 200 expressed genes\n# - Mitochondrial ratio quality (based on visual inspection)\n# - Read counts\n\n# In[8]:\n\n\n# Remove unknown cell types\nanndata = anndata[~anndata.obs['cell_type'].isnull()]\n\n# Filter out to only 24hr observations (~90% of the data remains)\nanndata = anndata[anndata.obs['time_point'] == 24]\n\n# Restrict to genes in human genome\nannot = sc.queries.biomart_annotations(\n\"hsapiens\", [\"ensembl_gene_id\", \"hgnc_symbol\"]\n)\nannot = annot.dropna()\nanndata = anndata[:, anndata.var['ensembl_gene_id'].isin(annot['ensembl_gene_id'].values)]\n\n# Filter out genes that are expressed in less than 10 cells\nsc.pp.filter_genes(anndata, min_cells=10)\n\n# Filter out cells that express less than 200 genes\nsc.pp.filter_cells(anndata, min_genes=200)\n\n\n# In[9]:\n\n\n# Trim outliers by\n# 1. Mitochondrial ratio quality\n# 2. total read counts\n# 3. number of genes per cell\nanndata.var[\"mt\"] = anndata.var_names.str.startswith(\"MT-\")\nsc.pp.calculate_qc_metrics(\nanndata, qc_vars=[\"mt\"], percent_top=None, log1p=False, inplace=True\n)\n\nanndata.var_names_make_unique()\n# Quality plots to identify outliers\nsc.pl.violin(\nanndata,\n[\"n_genes_by_counts\", \"total_counts\", \"pct_counts_mt\"],\njitter=0.3,\nmulti_panel=True,\nstripplot=False", "description": "This section showcases data preprocessing steps including filtering cells and genes based on various thresholds, calculating quality control metrics, and visualizing these metrics to identify outliers."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pandas as pd\nimport scipy.sparse\nimport scanpy as sc\n\n# In[10]:\n\n\n# define outliers and do the filtering for the sciplex data\n# with arbitrary cutoffs\nanndata.obs['outlier_mt'] = anndata.obs.pct_counts_mt > 60\nanndata.obs['outlier_total'] = anndata.obs.total_counts > 20000\nanndata.obs['outlier_ngenes'] = anndata.obs.n_genes_by_counts > 6000\n\nprint('%u cells with high %% of mitochondrial genes' % (sum(anndata.obs['outlier_mt'])))\nprint('%u cells with large total counts' % (sum(anndata.obs['outlier_total'])))\nprint('%u cells with large number of genes' % (sum(anndata.obs['outlier_ngenes'])))\n\nanndata = anndata[~anndata.obs['outlier_mt'], :]\nanndata = anndata[~anndata.obs['outlier_total'], :]\nanndata = anndata[~anndata.obs['outlier_ngenes'], :]", "description": "Demonstrates further preprocessing of the SciPlex data to filter outliers using specific criteria like mitochondrial gene percentage, total read counts, and the number of genes per cell. It follows up with a final gene filtering step."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport scanpy as sc\n\n# ## Goal: Understand the effects that the cancer drug candidate Pracinostat SB939 has on other target genes\n#\n# We choose a particular drug (Pracinostat (SB939)), which is currently in a Phase II clinical trial for treating T cell lymphoma. We start our analysis by looking at several summary statistics and visualizations of this drug. Then, we use `ChiRho` to estimate the causal effect of this drug on gene expression levels.\n#\n# Below we look at which pathway Pracinostat (SB939) targets.\n#\n\n# In[11]:\n\n\nanndata.obs[anndata.obs.product_name.str.contains(\"Pracin\")][['target']].drop_duplicates().reset_index(drop=True)\n\n\n# Histone deacetylase (HDAC) inhibitors are anti-cancer agents that can induce death, apoptosis, and cell cycle arrest in cancer cells Kim (2011). HDAC inhibitors have been shown to modify the activity of many transcription factors. Some of these include NCOA3, MYB, E2F1, KLF1, FEN1, GATA1, HNF4A, XRCC6, NF\u03baB, PCNA, TP53, RB1, Runx, SF1 Sp3, STAT, GTF2E1, TCF7L2, YY1 (Drummond 2005, Yang 2007). We focus on modeling the changes in expression of genes  associated with these transcription factors in response to Pracinostat (`target_genes`). We would like to understand which of the genes in `target_genes` are (1) most affected by Pracinostat, (2) upregulated or downregulated, and (3) have effects that vary across different cell types.\n\n# In[12]:\n\n\ndrug_target = \"Pracinostat (SB939)\"\ndrug_data = anndata[anndata.obs.product_name.str.contains(\"Pracinostat|Vehi\")]\nanndata = None # save on memory\n\n\n# To pick the target set of genes, we select from a combination of different gene sets. We select SIRT2 in `target_genes` below since it is a direct HDAC inhibitor, and validated as being upregulated by HDAC inhibitor drugs in Srivatsan (2020). We also select a list of the genes from Drummond (2005) and Yang (2007) that represent genes associated with transcription factors reported to be downstream of HDAC inhibitors. Finally, we use the Cistrome Data Browser (an epigenetic profile database) to select the \"top five genes\" potentially regulated by the enhancers. We select these \"top 5 genes\" by looking at the enhancers with the highest enriched H3K27ac signals for each cell line (based on ChIP-seq). This choice is motivated by the fact that an HDAC inhibitor will change the global acetylation levels. We assume that the top genes marked by H3K27ac are potentially regulated by the enhancers with the highest enriched H3K27ac signals in the same cell lines.\n\n# In[13]:\n\n\n# Genes from https://en.wikipedia.org/wiki/Histone_deacetylase#HDAC_inhibitors\nwiki_genes = [\n'NCOA3', 'MYB', 'E2F1', 'KLF1', 'FEN1', 'GATA1', 'HNF4A', 'XRCC6', 'NFKB1', 'PCNA', 'TP53', 'RB1', 'RUNX1', 'SF1', 'SP3', 'STAT1', 'GTF2E1', 'TCF7L2', 'YY1', 'SIRT2'\n]\n\n# http://cistrome.org/db/#/\n# Search \"h3k27ac\" and input each cell type into \"Biological Sources\" field\n# Select first study and top 5 from \"Get Putative Targets\"\nchipseq_a549_targets = [\n\"PSME2\", \"RNF31\", \"EMC9\", \"METTL26\", \"NPY4R\"\n]\n\nchipseq_k562_targets = [\n\"ZBTB4\", \"POLR2A\", \"UFD1\", \"CDC45\", 'SLC9A5' # SLC35G6 not in dataset\n]\n\nchipseq_mcf7_targets = [\n\"IDI1\", \"MAFG\", \"LAMTOR2\", \"UBQLN4\", \"RPS2\"\n]\n\n# Add SIRT2 gene to validate against Srivatsan (2020)\ntarget_genes = wiki_genes + chipseq_a549_targets + chipseq_k562_targets + chipseq_mcf7_targets + ['SIRT2']\n\n\n# ### Exploratory data analysis\n\n# In[14]:\n\n\n# Use pearson residuals for better gene visualization\n# see https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02451-7\nsc.experimental.pp.highly_variable_genes(\ndrug_data, flavor=\"pearson_residuals\", n_top_genes=10000\n)\n\n\n# In[15]:\n\n\nfig, ax = plt.subplots(1, figsize=(6, 5))\n\nhvgs = drug_data.var[\"highly_variable\"]\n\nax.scatter(\ndrug_data.var[\"mean_counts\"], drug_data.var[\"residual_variances\"], s=3, edgecolor=\"none\",\nlabel='Not highly variable genes'\n)\nax.scatter(\ndrug_data.var[\"mean_counts\"][hvgs],\ndrug_data.var[\"residual_variances\"][hvgs],\nc=\"tab:red\",\nlabel=\"Highly variable genes\",\ns=3,\nedgecolor=\"none\",\n)\nax.scatter(\ndrug_data.var[\"mean_counts\"][target_genes],\ndrug_data.var[\"residual_variances\"][target_genes],\nc=\"k\",\nlabel=\"Target genes\",\ns=10,\nedgecolor=\"none\",\n)\n\nax.set_xscale(\"log\")\nax.set_xlabel(\"mean expression\")\nax.set_yscale(\"log\")\nax.set_ylabel(\"residual variance\")\nax.spines[\"right\"].set_visible(False)\nax.spines[\"top\"].set_visible(False)\nax.yaxis.set_ticks_position(\"left\")\nax.xaxis.set_ticks_position(\"bottom\")\n\nax.legend()\n\n\n# In[16]:\n\n\n# Filter to highly variable genes\ndrug_data = drug_data[:, drug_data.var[\"highly_variable\"]]\n", "description": "Focuses on exploring the data related to Pracinostat (SB939), specifically looking into its target pathway. It includes retrieving unique values of target pathways from the dataset and visualizing variable genes for further analysis."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport numpy as np\nimport scanpy as sc\n\n# Let's look at how each cell clusters based on cell type and if the cell was given the drug or not using a Uniform Manifold Approximation and Projection (UMAP) embedding of the highly variable genes.\n\n# In[17]:\n\n\n# The UMAP function does a visualization using 'X' in the anndata object\n# We save the raw counts for later use\ndrug_data.layers[\"raw\"] = drug_data.X.copy()\n\n# Now, 'X' refers to pearson residuals\nsc.experimental.pp.normalize_pearson_residuals(drug_data)\n\n# Log1p normalize of the data (used for UMAP visual)\ndrug_data.layers[\"log1p\"] = np.log1p(\nsc.pp.normalize_total(drug_data, inplace=False)[\"X\"]\n)\n\nsc.pp.neighbors(drug_data, n_neighbors=30, n_pcs=30)\nsc.tl.umap(drug_data)\n\n\n# In the UMAP latent space, we see a clear clustering of cells around both cell type and if the cell was given the drug or not.\n\n# In[18]:\n\n\n# Visualize expression pattern per cell in UMAP\nsc.pl.umap(drug_data, layer='log1p',\ncolor=['cell_type', 'product_name'])", "description": "Describes the process of clustering cells based on treatment conditions and cell types using Uniform Manifold Approximation and Projection (UMAP) for dimensionality reduction on high-variance genes."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# In[23]:\n\n\ndef plot_average_expression(data, gene_names):\ndrug_expression_df = pd.DataFrame(data[:, gene_names].layers['log1p'].toarray(), columns=gene_names)\ndrug_expression_df['log10dose'] = np.log10(data.obs['dose'].values)\ndrug_expression_df = drug_expression_df.replace(-np.inf, 0)\ndata.obs['log10dose'] = drug_expression_df['log10dose'].values\ndrug_expression_df['cell_type'] = data.obs['cell_type'].values\navg_expression = drug_expression_df.groupby(by=['cell_type', 'log10dose']).mean().reset_index()\navg_expression = avg_expression.melt(id_vars=['cell_type', 'log10dose'])\navg_expression.rename({'variable': 'gene_name', 'value': 'avg_expression'}, inplace=True, axis=1)\n\nfig, ax = plt.subplots(3, 6, figsize=(24, 12))\n\nfor i, gene in enumerate(gene_names):\ncurr_ax = ax[i // 6][i % 6]\nif gene in avg_expression.gene_name.values:\nsns.lineplot(data=avg_expression[avg_expression.gene_name == gene],\nx='log10dose', y='avg_expression', hue='cell_type', ax=curr_ax, marker='o')\ncurr_ax.set_title(gene)\ncurr_ax.set_xlabel('Log Dose')\ncurr_ax.set_ylabel('Log Expression')\ncurr_ax.legend(loc='upper left')\nplt.tight_layout()\n\n\n# We plot the average normalized gene expression levels below. These plots represent the transcriptional variation diversity from different drug dosages across different cancer cell lines. Since we do not control for any other variables, the observed correlation can be interpreted as a composition of biological signal, confounding factors, and sequencing noise. Thus, the observed correlations are not necessarily causal. In the next section, we attempt to remove the confounding factors and de-noise the sequencing data so that we can interpret the effects causally.\n\n# In[24]:\n\n", "description": "This example showcases how to plot average gene expression levels across different cell types and drug dosages. It involves creating custom plotting functions to analyze and visualize how treatment with Pracinostat affects gene expression in stratified groups."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport math\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.nn import PyroModule\n\n# ## Causal Probabilistic Program\n#\n# ### Model Description\n# In the plots above, we did not control for technical covariates (e.g., cell plate, replicate number, etc.). Here we fit a generalized linear model with a Poission likelihood to account for the heavy-tailed count nature of the data. We also include interactions between genes and confounders (e.g., cell type, read depth, etc.) to understand how treatment effects vary across cell types and experimental conditions, and model unobserved confounders. Our model is defined as follows:\n#\n# $$\n# \\begin{split}\n#     Z_{nk} &\\sim \\mathcal{N}(0, 1), \\quad{1 \\leq k \\leq K} \\quad (\\text{$K$ unobserved confounders for cell}) \\\\\n#     \\tilde{X}_n &= (X_n, \\ Z_n) \\in \\mathbb{R}^{p + K} \\quad (\\text{observed and unobserved confounders}) \\\\\n#     c_g &\\sim \\mathcal{N}(0, 1) \\quad (\\text{intercept}) \\\\\n#     \\tau_{g} &\\sim \\mathcal{N}(0, 1) \\quad (\\text{additive treatment effect}) \\\\\n#     \\theta_{gi} &\\sim \\mathcal{N}(0, 1 / \\sqrt{p + K}), \\quad{1 \\leq i \\leq p + K} \\quad (\\text{additive confounder effect}) \\\\\n#     \\eta_{gi} &\\sim \\mathcal{N}(0, 1 / \\sqrt{p + K}), \\quad{1 \\leq i \\leq p + K} \\quad (\\text{confounder-treatment interaction effect}) \\\\\n#     \\log \\mu_{ng} &= c_g + \\tau_{g} T_n + \\theta_{g}^T \\tilde{X}_n + \\eta_{g}^T (\\tilde{X}_n \\odot T_n) \\quad (\\text{Poisson rate}) \\\\\n#     Y_{ng} &\\sim \\text{Poisson}(\\mu_{ng}) \\quad (\\text{Poisson likelihood}) \\\\\n# \\end{split}\n# $$\n# where $\\odot$ denotes elementwise multiplication, $p$ the number of confounders, $T_n$ treatment, $X_n$ observed confounders, and $Y_{ng}$ gene expression counts for cell $n$ and gene $g$.\n#\n# Note: our inclusion of the $K$ latent confounders for each cell is similar to regressing out the top principal components from the gene-expression matrix. However, our approach, which allows us to account for the uncertainty of not observing the latent confounders, is known as probabilistic principal components analysis; see Tipping (1999) for details.\n\n# In[25]:\n\n\nclass DrugPerturbationModel(PyroModule):\ndef __init__(self, num_cells: int, num_genes: int, num_observed_confounders: int, num_latent_confounders: int = 3):\nsuper().__init__()\nself.num_cells = num_cells\nself.num_genes = num_genes\nself.num_observed_confounders = num_observed_confounders\nself.num_latent_confounders = num_latent_confounders\nself.num_confounders = num_observed_confounders + num_latent_confounders\nself.cell_plate = pyro.plate(\"cells\", self.num_cells, dim=-2)\nself.gene_plate = pyro.plate(\"genes\", self.num_genes, dim=-1)\n\ndef forward(self, mask: bool = False):\nprior_scale = 1 / math.sqrt(self.num_confounders)\npropensity_weights = pyro.sample(\"propensity_weights\", dist.Normal(0, prior_scale).expand((self.num_confounders, )).to_event(1))\n\n# Sample regression weights for each gene\nwith self.gene_plate:\nintercept = pyro.sample(\"intercept\", dist.Normal(0, 1))\ntheta_confounders = pyro.sample(\"theta_confounders\", dist.Normal(0, prior_scale).expand((self.num_confounders,)).to_event(1)) # G x F\ntheta_drug = pyro.sample(\"theta_drug\", dist.Normal(0, 1))\ntheta_confounders_drug = pyro.sample(\"theta_confounders_drug\", dist.Normal(0, prior_scale).expand((self.num_confounders,)).to_event(1)) # G x F\n\n# Compute likelihood\nwith self.cell_plate:\n# Sample treatment and confounders (remove from log_prob since we'll always conditioned on them when mask = False)\nlatent_confounders = pyro.sample(\"latent_confounders\", dist.Normal(0, 1).expand((self.num_latent_confounders,)).to_event(1))\nobserved_confounders = pyro.sample(\"observed_confounders\", dist.Normal(0, 1).expand((self.num_observed_confounders,)).mask(mask).to_event(1))\nobserved_confounders = observed_confounders.expand(latent_confounders.shape[:-1] + observed_confounders.shape[-1:])\nconfounders = torch.cat([observed_confounders, latent_confounders], dim=-1)  # torch.cat doesn't broadcast\n\ntreatment_mean = torch.einsum(\"...ngp,...p->...ng\", confounders, propensity_weights)\nT = torch.relu(pyro.sample(\"drug\", dist.Normal(treatment_mean, 1)))\nwith self.gene_plate:\n# Confounding effects\nconfounder_variation = intercept + torch.einsum(\"...ngp,...gp->...ng\", confounders, theta_confounders) # N x G\n\n# Drug dose treatment effects\ntreatment_variation = theta_drug * T + torch.einsum(\"...ngp,...ng,...gp->...ng\", confounders, T, theta_confounders_drug) # N x G\nmean = confounder_variation + treatment_variation # N x G\nexpressions = pyro.sample(\"gene_expression\", dist.Poisson(rate=torch.exp(mean)))\nreturn expressions\n", "description": "Explains modeling the effects of drug treatment using causal probabilistic programming with Pyro. It defines a Pyro model for drug perturbations, incorporating factors like latent confounders and treatment effects."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport math\nimport torch\nimport numpy as np\nimport pandas as pd\nimport pyro\nfrom pyro.infer.autoguide import AutoNormal\nfrom pyro.nn import PyroModule\nfrom pyro.infer import Trace_ELBO\n\n# ### Fitting observed data\n#\n# We further process the data for modeling by (1) one-hot encoding categorical confounders, (2) converting dosages to log dosage amounts, (3) splitting the data in training and test.\n\n# In[26]:\n\n\n# Restrict to top 1000 genes or those in target_genes\nis_top1k_mask_or_target = sc.experimental.pp.highly_variable_genes(\ndrug_data, flavor=\"pearson_residuals\", n_top_genes=1000,\ninplace=False\n)['highly_variable'].values | drug_data.var.index.isin(target_genes)\n\ndrug_data = drug_data[:, is_top1k_mask_or_target]\n\n\n# In[27]:\n\n\n# Make data matrices\nX_confounders = pd.get_dummies(drug_data.obs[['cell_type', 'replicate', 'culture_plate', 'pcr_plate', 'well_oligo']], dtype=np.int32).reset_index(drop=True)\n\n# Control for sequencing depth\nlog_umi = np.log(drug_data.obs['n.umi'].values)\nscaled_log_umi = (log_umi - np.mean(log_umi)) / log_umi.std()\nX_confounders['depth_proxy'] = scaled_log_umi\n\n# For each drug, input log dose as a feature\nT_all = pd.get_dummies(drug_data.obs[['product_name']], dtype=np.int32).reset_index(drop=True)\nT_all = T_all.mul(np.log10(drug_data.obs['dose'].values + 1), axis=0)\n\n# Get treatment, confounders, response for all gene expressions\nX_drug_control = torch.tensor(X_confounders.values, dtype=torch.float32)\nT_drug_control = torch.tensor(T_all[f'product_name_{drug_target}'].values, dtype=torch.float32)\nY_drug_control = torch.tensor(drug_data.layers['raw'].toarray(), dtype=torch.float32)\n\n# Split into training and test set\nN_drug_control = X_drug_control.shape[0]\ntrain_mask = torch.tensor(np.random.choice([True, False], size=N_drug_control, p=[0.8, 0.2]))\nX_train = X_drug_control[train_mask]\nT_train = T_drug_control[train_mask]\nY_train = Y_drug_control[train_mask]\nX_test = X_drug_control[~train_mask]\nT_test = T_drug_control[~train_mask]\nY_test = Y_drug_control[~train_mask]\n\n\n# In[28]:\n\n\nclass ConditionedDrugModel(PyroModule):\ndef __init__(self, model: DrugPerturbationModel):\nsuper().__init__()\nself.model = model\n\ndef forward(self, X, T, Y, **kwargs):\nwith condition(data=dict(observed_confounders=X[:, None, :], drug=T[..., None], gene_expression=Y)):\nreturn self.model(**kwargs)\n\n\n# We fit our model using stochastic variational inference with a mean-field variational family.\n\n# In[29]:\n\n\nmodel_train = ConditionedDrugModel(\nDrugPerturbationModel(num_cells=Y_train.shape[0], num_genes=Y_train.shape[1], num_observed_confounders=X_train.shape[1])\n)\n\nguide_train = AutoNormal(model_train)\nelbo = pyro.infer.Trace_ELBO()(model_train, guide_train)\n\n# initialize parameters\nelbo(X=X_train, T=T_train, Y=Y_train)\n\nadam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n\n# Do gradient steps\nfor step in range(2000):\nadam.zero_grad()\nloss = elbo(X=X_train, T=T_train, Y=Y_train)\nloss.backward()\nadam.step()\nif step % 250 == 0:\nprint(\"[iteration %04d] loss: %.4f\" % (step, loss))", "description": "Covers fitting the defined causal probabilistic program to observed data. This includes preprocessing the dataset for modeling, defining a conditioned model wrapper, running stochastic variational inference, and performing optimization steps."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport numpy as np\nimport torch\nimport pyro\nfrom pyro.nn import PyroModule\nfrom pyro.infer import Trace_ELBO, Predictive\nfrom pyro.infer.autoguide import AutoGuideList, AutoNormal\n\n# ### Predictions on unseen cells\n#\n# As we see below, our predictions of gene expressions are quite poor. However, this is not very suprising because we are using a linear model, and not conditioning on more cell and gene features.\n\n# In[30]:\n\n\nmodel_test_obs = ConditionedDrugModel(\nDrugPerturbationModel(num_cells=Y_test.shape[0], num_genes=Y_test.shape[1], num_observed_confounders=X_test.shape[1])\n)\n\nguide_train.eval()\nguide_test_obs = pyro.infer.autoguide.AutoGuideList(model_test_obs)\nguide_test_obs.append(pyro.poutine.block(guide_train, hide=['latent_confounders', 'latent_confounders_unconstrained', 'cells', 'genes']))\nguide_test_obs.append(AutoNormal(pyro.poutine.block(model_test_obs, expose=['latent_confounders', 'cells', 'genes'])))\n\nelbo = pyro.infer.Trace_ELBO()(model_test_obs, guide_test_obs)\n\n# initialize parameters\nelbo(X=X_test, T=T_test, Y=Y_test)\n\nadam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n\n# Do gradient steps\nfor step in range(2000):\nadam.zero_grad()\nloss = elbo(X=X_test, T=T_test, Y=Y_test)\nloss.backward()\nadam.step()\nif step % 250 == 0:", "description": "Describes how to make predictions on unseen cells using the causal model by processing test data, reusing the trained variational guide, and performing inference to estimate gene expression levels."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\n\n# In[31]:\n\n\npredictive = pyro.infer.Predictive(model_test_obs, guide=guide_test_obs, num_samples=250)\npredictive_samples = predictive(X=X_test, T=T_test, Y=None)\ntest_avg = predictive_samples['gene_expression'].mean(dim=0)\n\n\n# In[42]:\n\n\n# Compare posterior mean predictions across 13 genes\nfig, ax = plt.subplots(3, 6, figsize=(20, 10))\n\nfor i, gene in enumerate(target_genes):\ncurr_ax = ax[i // 6][i % 6]\ngene_idx = drug_data.var_names.get_loc(gene)\ncurr_ax.scatter(test_avg[:, gene_idx], Y_test[:, gene_idx], alpha=0.5)\ncurr_ax.set_title(gene)\ncurr_ax.set_xlabel('Predicted Expression')\ncurr_ax.set_ylabel('Actual Expression')\n\nplt.tight_layout()\n\n\n# Often, we are not able to accurately predict gene expressions for individual cells. However, many papers evaluate at the bulk level instead. That is, they evaluate how well the model predicts the average expression of each gene across a collection of similar cells. We evaluate our model in this way below since it is more relevant to the downstream task of understanding the effects of a drug at the tissue level.\n\n# In[43]:\n\n\n# Compare posterior mean predictions across all genes\n# Average gene expression comparison https://www.nature.com/articles/s41592-019-0494-8/figures/2\n\n# Segment by cell type\nfig, ax = plt.subplots(1, 3, figsize=(13, 4))\nfor i, cell_type in enumerate(['A549', 'MCF7', 'K562']):\ncell_type_mask = torch.tensor(drug_data[~train_mask.numpy()].obs['cell_type'].values == cell_type)\ntest_avg_bulk = test_avg[cell_type_mask].mean(dim=0)\n\nax[i].scatter(test_avg_bulk, Y_test[cell_type_mask].mean(dim=0), alpha=0.5)\nax[i].set_xlabel('Average of posterior Predicted Expression across cells')\nax[i].set_ylabel('Actual average Expression across cells')\nax[i].set_title(f'{cell_type} Cells (Correlation={pearsonr(test_avg_bulk, Y_test[cell_type_mask].mean(dim=0))[0]:.3f})')\nax[i].set_xscale('log')\nax[i].set_yscale('log')\nplt.tight_layout()\n\n\n# Once we look at how our model performs at the bulk level, we see that the predictions are quite good.\n\n# In[44]:\n\n\n# Group cells by log dosage and then compare average expressions with the model\nfig, ax = plt.subplots(1, 5, figsize=(20, 5))\nfor i, log_dose_level in enumerate([0., 1., 2., 3., 4.]):\ndose_level_mask = torch.isclose(T_test, torch.tensor(log_dose_level), atol=.2)\ntest_avg_bulk = test_avg[dose_level_mask].mean(dim=0)\n\nax[i].scatter(test_avg_bulk, Y_test[dose_level_mask].mean(dim=0), alpha=0.5)\nax[i].set_xlabel('Average of posterior Predicted Expression across cells')\nax[i].set_ylabel('Actual average Expression across cells')\nax[i].set_title(f'Log Dosage={log_dose_level} (Correlation={pearsonr(test_avg_bulk, Y_test[dose_level_mask].mean(dim=0))[0]:.2f})')\nax[i].set_xscale('log')\nax[i].set_yscale('log')", "description": "Focuses on evaluating model performance both at individual cell level and in bulk by comparing predicted and actual gene expression levels. It demonstrates the evaluation of the model's predictions using Pearson correlation to assess model accuracy."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nfrom pyro.nn import PyroModule\nfrom pyro.infer import Predictive\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.interventional.handlers import do\nfrom chirho.indexed.ops import IndexSet, gather\nimport pandas as pd\nimport seaborn as sns\n\n# ## Computing causal queries\n#\n# For each gene in `target_genes`, we would like to estimate the causal effect of Pracinostat (SB939) on the gene's expression level at each dosage level $T \\in \\{0, 10^1, 10^2, 10^3, 10^4\\}$. Since we only observe the expression level of each cell at a particular dosage, we impute the values of the remaining dosages using our model.\n\n# ### Causal Query: Average treatment effect (ATE)\n#\n# The average treatment effect summarizes, on average, how much the drug changes the expression level of a gene, $ATE(t) = \\mathbb{E}[Y|do(T=t)] - \\mathbb{E}[Y|do(T=0)]$. The `do` notation indicates that the expectations are taken according to *intervened* versions of the model, with $T$ set to a particular value. Note from our [tutorial](tutorial_i.ipynb) that this is different from conditioning on $T$ in the original `causal_model`, which assumes $X$ and $T$ are dependent.\n#\n# In this setting, the ATE tells us how much lower or higher a gene's expression level is when the drug is administered at a particular dosage level relative to when the drug is not administered. Here, we are interested in the average over the entire population of cells. Other estimands, such as the conditional average treatment effect, may be interested in the average effect for cells with particular attributes of interest.\n#\n# To implement this query in ChiRho, we extend our `DrugPerturbationModel` model by applying two interventions, `do(actions={\"drug\":0})` and `do(actions={\"drug\": t})`, and then sampling jointly from counterfactual worlds using the `MultiWorldCounterfactual` handler. Recall from the [tutorial](tutorial_i.ipynb) that the `MultiWorldCounterfactual` handler modifies the execution of the causal model to sample jointly from the observational and all counterfactual worlds induced by an intervention.\n\n# In[45]:\n\n\nclass DrugPerturbationATE(pyro.nn.PyroModule):\ndef __init__(self, model: DrugPerturbationModel, X_pop: torch.tensor):\nsuper().__init__()\nself.model = model\n# sample population confounders to average over\nself.X_pop = X_pop\n\ndef forward(self, dose_levels: tuple):\ndose_levels = (torch.tensor(0.),) + tuple(torch.as_tensor(dose_level) for dose_level in dose_levels)\n\nwith MultiWorldCounterfactual(), do(actions={\"drug\": dose_levels}), \\\ncondition(data=dict(observed_confounders=self.X_pop[:, None, :])):\nexpressions = self.model()\nexpressions_dose = gather(expressions, IndexSet(drug=set(range(2, len(dose_levels) + 1)))) # expressions predicted by model at doses > 0\nexpressions_vehicle = gather(expressions, IndexSet(drug={1})) # expressions predicted by model at dose 0 (vehicle)\nreturn pyro.deterministic(\"ATEs\", (expressions_dose - expressions_vehicle).mean(dim=-2))\n\nate_model = DrugPerturbationATE(\nDrugPerturbationModel(num_cells=Y_train.shape[0], num_genes=Y_train.shape[1], num_observed_confounders=X_train.shape[1]),\nX_pop=X_train\n)\nate_predictive = pyro.infer.Predictive(ate_model, guide=guide_train, num_samples=250, return_sites=(\"ATEs\",))\n\nlog_dose_levels = (1., 2., 3., 4.)\nate_samples = ate_predictive(log_dose_levels)['ATEs'].squeeze()\n\n\n# In[46]:\n\n\n# Issues w/ shuffling\ntarget_mask_reindexed = drug_data.var.index.isin(target_genes)\ntarget_genes_reindexed = drug_data.var.index[target_mask_reindexed]\n\n\n# In[47]:\n\n\nfig, ax = plt.subplots(2, 2, figsize=(15, 8))\n\nfor i, dose_level in enumerate(log_dose_levels):\ncurr_ax = ax[i // 2][i % 2]\nsns.boxplot(pd.DataFrame(ate_samples[:, i, target_mask_reindexed], columns=target_genes_reindexed), color='green', ax=curr_ax)\ncurr_ax.set_title(f'Average Treatment Effect (Dosage = $10^{int(dose_level)}$)')\ncurr_ax.axhline(0, color='black', linestyle='--', label='Zero treatment effect')\ncurr_ax.set_xlabel('Target Gene')\nfor tick in curr_ax.get_xticklabels():\ntick.set_rotation(45)\ncurr_ax.set_ylabel('Average Treatment Effect')\ncurr_ax.legend()\ncurr_ax.set_ylim(-.5, .5)\nsns.despine()\n\nplt.tight_layout()", "description": "Illustrates computing the Average Treatment Effect (ATE) of Pracinostat using counterfactual reasoning within the causal inference framework. It involves extending the model to perform interventions and computing ATE across different dosage levels."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/sciplex.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nfrom pyro.nn import PyroModule\nfrom pyro.infer import Predictive\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.interventional.handlers import do\nfrom chirho.indexed.ops import IndexSet\nimport pandas as pd\nimport seaborn as sns\n\n# ### Causal Query: Conditional average treatment effect (CATE)\n#\n# The ATE summarizes how much the drug changes the expression level of a gene on average, but certain types of cells might respond differently to the drug. In this setting we wish to compute the *conditional average treatment effect*, $CATE(t) = \\mathbb{E}[Y|do(T=t), X=x] - \\mathbb{E}[Y|do(T=0), X=x]$, which controls for cell-specific confounders $X$.\n#\n# In words, in this setting the CATE tells us how much lower or higher a gene's expression level is when the drug is administered at a particular dosage level relative to when the drug is not administered, *for cells with the same set of confounders*. Here, we are interested in the average only over that subpopulation of cells with the same value of the confounders.\n\n# In[48]:\n\n\nclass DrugPerturbationCATE(pyro.nn.PyroModule):\ndef __init__(self, model: DrugPerturbationModel):\nsuper().__init__()\nself.model = model\n\ndef forward(self, dose_levels: tuple, X: torch.tensor, T: torch.tensor, Y: torch.tensor):\ndose_levels = (torch.tensor(0.),) + tuple(torch.as_tensor(dose_level) for dose_level in dose_levels)\n\nwith MultiWorldCounterfactual(), do(actions={\"drug\": dose_levels}), \\\ncondition(data={\"observed_confounders\": X[:, None, :], \"drug\": T[..., None], \"gene_expression\": Y}):\nexpressions = self.model()\nexpressions_dose = gather(expressions, IndexSet(drug=set(range(2, len(dose_levels) + 1)))) # expressions predicted by model at doses > 0\nexpressions_vehicle = gather(expressions, IndexSet(drug={1})) # expressions predicted by model at dose 0 (vehicle)\nreturn pyro.deterministic(\"CATEs\", (expressions_dose - expressions_vehicle))\n\n\n# In[49]:\n\n\nlog_dose_levels = (1., 2., 3., 4.)\ncell_types = ['A549', 'MCF7', 'K562']\n\n# Conditional average treatment effect for each cell\ncate_model = DrugPerturbationCATE(\nDrugPerturbationModel(num_cells=X_test.shape[0], num_genes=Y_test.shape[1], num_observed_confounders=X_test.shape[1])\n)\n\ncate_predictive = pyro.infer.Predictive(cate_model, guide=guide_test_obs, num_samples=250, return_sites=(\"CATEs\",))\ncate_for_each_cell = cate_predictive(log_dose_levels, X_test, T_test, Y_test)['CATEs'].squeeze()\n\n# To get conditional average treatment by cell type, we average cate_predictive across the subset of cells that are of each cell type\ncate_samples = {}\nfor cell_type in cell_types:\ncell_type_mask = torch.tensor(drug_data.obs['cell_type'].values == cell_type)[~train_mask]\n\n# condition on cell type, average over other covariates by averaging over cells\n# cate_samples is matrix of size num posterior samples x num dosages x num cells x num genes\ncate_samples[cell_type] = cate_for_each_cell[:, :, cell_type_mask, :].mean(dim=-2)\n\n\n# In[50]:\n\n\nfor cell_type in cell_types:\nfig, ax = plt.subplots(2, 2, figsize=(15, 8))\n\nfor i, dose_level in enumerate(log_dose_levels):\ncurr_ax = ax[i // 2][i % 2]\nsns.boxplot(pd.DataFrame(cate_samples[cell_type][:, i, target_mask_reindexed], columns=target_genes_reindexed), color='green', ax=curr_ax)\ncurr_ax.set_title(f'Conditional Average Treatment Effect (Cell Type = {cell_type}, Dosage = $10^{int(dose_level)}$)')\ncurr_ax.axhline(0, color='black', linestyle='--', label='Zero treatment effect')\ncurr_ax.set_xlabel('Target Gene')\nfor tick in curr_ax.get_xticklabels():\ntick.set_rotation(45)\ncurr_ax.set_ylabel('Cell Type Conditional Average Treatment Effect')\ncurr_ax.set_ylim(-.5, .5)\ncurr_ax.legend()\nsns.despine()\n", "description": "Demonstrates computing the Conditional Average Treatment Effect (CATE) to understand how the effects of Pracinostat vary across cell types and dosage levels. It involves executing counterfactual queries for specific subpopulations defined by cell marker expressions."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport os\nimport torch\nimport pyro\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npyro.clear_param_store()\npyro.set_rng_seed(1234)\npyro.settings.set(module_local_params=True)\nsmoke_test = ('CI' in os.environ)\nmax_epochs = 10 if smoke_test else 1000\nnum_samples = 10 if smoke_test else 10000\n\n# ## Setup\n\n# Here, we install the necessary Pytorch, Pyro, and ChiRho dependencies for this example.\n\n# In[11]:\n\n\nget_ipython().run_line_magic('reload_ext', 'autoreload')\nget_ipython().run_line_magic('autoreload', '2')\nget_ipython().run_line_magic('pdb', 'off')\n\nfrom typing import Dict, List, Optional, Tuple, Union, TypeVar\n\nimport os\nimport torch\nimport pyro\nimport pyro.distributions as dist\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pytorch_lightning as pl\n\nfrom pyro.nn import PyroModule, PyroSample\nfrom pyro.poutine import replay, trace\n\nfrom pyro.infer.autoguide import AutoDelta,  AutoNormal\nfrom pyro.infer import SVI, Predictive\n\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather\nfrom chirho.interventional.handlers import do\nfrom chirho.observational.handlers import condition\n\npyro.clear_param_store()\npyro.set_rng_seed(1234)\npyro.settings.set(module_local_params=True)\n\nsmoke_test = ('CI' in os.environ)\nmax_epochs = 10 if smoke_test else 1000\nnum_samples = 10 if smoke_test else 10000", "description": "Setup of the environment including installation of necessary libraries and setting seed for reproducibility."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport matplotlib.pyplot as plt\nDATA_URL = \"https://raw.githubusercontent.com/rugg2/rugg2.github.io/master/lalonde.csv\"\ndata = pd.read_csv(DATA_URL)\ndata[\"re75\"] = data[\"re75\"] / 1000\ndata[\"re78\"] = data[\"re78\"] / 1000 + 1e-6\ndata = data.rename(columns={\"educ\": \"education\", \"hispan\": \"hispanic\"})\ncovariates_names = [\"education\", \"age\", \"re75\", \"black\", \"hispanic\", \"married\", \"nodegree\"]\ndf = data[[\"treat\", *covariates_names, \"re78\"]]\ncovariates_obs = torch.tensor(df[covariates_names].values).float()\ntraining_obs = torch.tensor(df[\"treat\"].values).float()\nearnings_obs = torch.tensor(df[\"re78\"].values).float()\n\n# Load the data\nDATA_URL = \"https://raw.githubusercontent.com/rugg2/rugg2.github.io/master/lalonde.csv\"\ndata = pd.read_csv(DATA_URL)\n\n# Convert the data to the right format\ndata[\"re75\"] = data[\"re75\"] / 1000\n# Add a small constant to avoid log(0) in the model\ndata[\"re78\"] = data[\"re78\"] / 1000 + 1e-6\ndata = data.rename(columns={\"educ\": \"education\", \"hispan\": \"hispanic\"})\n\n# Define the covariates\ncovariates_names = [\"education\", \"age\", \"re75\", \"black\", \"hispanic\", \"married\", \"nodegree\"]\n\n# Extract treatment, covariates and earnings from the dataframe\ndf = data[[\"treat\", *covariates_names, \"re78\"]]\n\n# Convert to tensors\ncovariates_obs = torch.tensor(df[covariates_names].values).float()\ntraining_obs = torch.tensor(df[\"treat\"].values).float()\nearnings_obs = torch.tensor(df[\"re78\"].values).float()\n\n\n# In[4]:\n\n\n# Visualize the data\nsns.pairplot(df[[\"treat\", \"education\", \"age\", \"re75\", \"re78\"]], hue=\"treat\", diag_kind=\"hist\")", "description": "Loading and preprocessing of data, followed by visualisation of the data using seaborn."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "df = data[[\"treat\", *covariates_names, \"re78\"]]\n\n# Evaluate what our answer would be if we just naively predicted the average earnings of treated and untreated individuals,\n# without accounting for the potential confounders.\ntreated_individuals = df[df[\"treat\"] == 1]\nuntreated_individuals = df[df[\"treat\"] == 0]\n\nnaive_prediction = (treated_individuals[\"re78\"].mean() - untreated_individuals[\"re78\"].mean())", "description": "Naive prediction approach ignoring covariates for the effect of the job training program."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.nn import PyroModule, PyroSample\nfrom chirho.observational.handlers import condition\nfrom typing import Optional\npyro.clear_param_store()\nclass BayesianBackdoor(PyroModule):\n    pass\n    # Placeholder for the BayesianBackdoor class definition\n\nfrom typing import Dict, List, Optional, Tuple, Union, TypeVar\ncovariates_obs = torch.tensor([[0.]])  # Placeholder\nclass ConditionedBayesianBackdoor(PyroModule):\n    pass  # Placeholder for the ConditionedBayesianBackdoor class definition\n\nclass BayesianBackdoor(PyroModule):\nzero: torch.Tensor\none: torch.Tensor\n\ndef __init__(self, d_covariates=7):\nsuper().__init__()\nself.d_covariates = d_covariates\nself.register_buffer(\"zero\", torch.tensor(0.))\nself.register_buffer(\"one\", torch.tensor(1.))\nself.register_buffer(\"loc_covariates_loc\", torch.tensor([10., 35., 15.] + [1.] * (self.d_covariates - 3)))\n\n@PyroSample\ndef loc_covariates(self):\nreturn dist.Normal(self.loc_covariates_loc, self.one).to_event(1)\n\n@PyroSample\ndef variances_covariates(self):\nreturn dist.HalfCauchy(self.one).expand([self.d_covariates]).to_event(1)\n\n@PyroSample\ndef lower_cholesky_covariates(self):\nreturn dist.LKJCholesky(self.d_covariates, self.one)\n\n@PyroSample\ndef weights_training(self):\nreturn dist.Normal(self.zero, 1. / self.loc_covariates_loc[..., None]).expand([self.d_covariates, 1]).to_event(2)\n\n@PyroSample\ndef bias_training(self):\nreturn dist.Normal(self.zero, self.one * 10.).expand([1]).to_event(1)\n\n@PyroSample\ndef weights_earnings(self):\nreturn dist.Normal(self.zero, 1. / self.loc_covariates_loc[..., None]).expand([self.d_covariates, 2]).to_event(2)\n\n@PyroSample\ndef bias_earnings(self):\nreturn dist.Normal(self.zero, self.one * 10.).expand([2]).to_event(1)\n\n@PyroSample\ndef variance_earnings(self):\nreturn dist.HalfCauchy(self.one * 10.).expand([2]).to_event(1)\n\ndef forward(self):\n\n# Sample covariates from a multivariate normal distribution\nscale_tril = torch.diag_embed(self.variances_covariates.sqrt()) @ self.lower_cholesky_covariates\ncovariates = pyro.sample(\"covariates\", dist.MultivariateNormal(self.loc_covariates, scale_tril=scale_tril))\n\n# Sample training (treatment) from a logistic function of the covariates\nlogit_training = torch.einsum(\"...a,...ab->...b\", covariates, self.weights_training) + self.bias_training\ntraining = (pyro.sample(\"training\", dist.Bernoulli(torch.special.expit(logit_training[..., 0])))).long()\n\n# Sample earnings (outcome) from a linear Gaussian function of the covariates and the treatment\nloc_earnings = torch.einsum(\"...a,...ab->...b\", covariates, self.weights_earnings) + self.bias_earnings\nloc_earnings = torch.where(training == 1, loc_earnings[..., 1], loc_earnings[..., 0])\nvariance_earnings = torch.where(training == 1, self.variance_earnings[..., 1], self.variance_earnings[..., 0])\nearnings = pyro.sample(\"earnings\", dist.FoldedDistribution(dist.Normal(loc_earnings, variance_earnings)))\n\nreturn covariates, training, earnings\n\n\nclass ConditionedBayesianBackdoor(PyroModule):\n\ndef __init__(self, causal_model: BayesianBackdoor, n: Optional[int] = None):\nsuper().__init__()\nself.causal_model = causal_model\nself.n = n\n\ndef forward(self, covariates_obs=None, training_obs=None, earnings_obs=None):\n\nn = covariates_obs.shape[0] if covariates_obs is not None else self.n\n\nself.causal_model.loc_covariates\nself.causal_model.variances_covariates\nself.causal_model.lower_cholesky_covariates\nself.causal_model.weights_training\nself.causal_model.bias_training\nself.causal_model.weights_earnings\nself.causal_model.bias_earnings\nself.causal_model.variance_earnings\n\nwith pyro.plate(\"data\", n, dim=-1):\nwith condition(data={\"covariates\": covariates_obs, \"training\": training_obs, \"earnings\": earnings_obs}):\nreturn self.causal_model()\n", "description": "Definition and instantiation of a Bayesian model with Pyro to encode causal relationships and assumptions."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "def plot_predictive(model, covariates_obs, training_obs, earnings_obs, guide=None, compare_source=True):\n    pass  # Placeholder for the plot_predictive function definition\nimport pandas as pd\nimport seaborn as sns\nimport torch\nimport matplotlib.pyplot as plt\ncovariates_obs = torch.tensor([[0.]])  # Placeholder\nbayesian_backdoor = ConditionedBayesianBackdoor(BayesianBackdoor())  # Assuming previous definitions\ncovariates_names = [\"education\", \"age\", \"re75\", \"black\", \"hispanic\", \"married\", \"nodegree\"]  # Assuming previously defined variables\n\ndef plot_predictive(model, covariates_obs, training_obs, earnings_obs, guide=None, compare_source=True):\n\nif guide:\nguide_tr = trace(guide).get_trace(covariates_obs, training_obs, earnings_obs)\nmodel_tr = trace(replay(model, trace=guide_tr)).get_trace(covariates_obs, training_obs, earnings_obs)\nelse:\nmodel_tr = trace(model).get_trace(covariates_obs, training_obs, earnings_obs)\n\n\ncovariates = model_tr.nodes['covariates']['value']\ntraining = model_tr.nodes['training']['value'][:, None]\n\nearnings = model_tr.nodes['earnings']['value'][..., :, None]\n\nsamples = torch.concat((training, covariates, earnings), dim=1).detach().numpy()\n\npredictive_df = pd.DataFrame(samples, columns=[\"treat\", *covariates_names, \"re78\"]).astype({\"treat\":\"int8\"})\n\ndata_copy = df.copy()\ndata_copy[\"source\"] = \"data\"\n\npredictive_copy = predictive_df.copy()\npredictive_copy[\"source\"] = \"predictive\"\n\nif compare_source:\n# Note that `.sample(frac=1).reset_index(drop=True)` shuffles the rows to minimize overplotting problems\nsns.pairplot(pd.concat((data_copy, predictive_copy), ignore_index=True)[[\"treat\", \"education\", \"age\", \"re75\", \"re78\", \"source\"]].sample(frac=1).reset_index(drop=True), hue=\"source\", diag_kind=\"hist\", plot_kws=dict(alpha=0.5))\nelse:\nsns.pairplot(predictive_df[[\"treat\", \"education\", \"age\", \"re75\", \"re78\"]], hue=\"treat\", diag_kind=\"hist\")\n\n\n# In[8]:\n\n", "description": "Visualisation of samples from the model to inspect the implications of modeling decisions."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nfrom chirho.counterfactual.handlers import MultiWorldCounterfactual\nfrom chirho.indexed.ops import IndexSet, gather\nfrom chirho.interventional.handlers import do\npyro.clear_param_store()\nclass BayesianBackdoorSATE(pyro.nn.PyroModule):\n    pass  # Placeholder for BayesianBackdoorSATE class definition\n\nclass BayesianBackdoorSATE(pyro.nn.PyroModule):\n\ndef __init__(self, causal_model: BayesianBackdoor):\nsuper().__init__()\nself.conditioned_model = ConditionedBayesianBackdoor(causal_model)\n\ndef forward(self, covariates_obs, training_obs, earnings_obs):\n\n# Sample jointly from observational and counterfactual distributions\nwith MultiWorldCounterfactual(), do(actions={\"training\": 1. - training_obs}):\n_, _, earnings = self.conditioned_model(covariates_obs, training_obs, earnings_obs)\n\n# Evaluate the sample average treatment effect\nearnings_cf = gather(earnings, IndexSet(training={1}))\nearnings_f = gather(earnings, IndexSet(training={0}))\nearnings_1 = torch.where(training_obs == 1, earnings_f, earnings_cf)\nearnings_0 = torch.where(training_obs == 0, earnings_f, earnings_cf)\nites = earnings_1 - earnings_0\nreturn pyro.deterministic(\"SATE\", torch.mean(ites, dim=-1, keepdim=True), event_dim=0)\n\nbayesian_backdoor_sate = BayesianBackdoorSATE(BayesianBackdoor())", "description": "Extension of the BayesianBackdoor model to compute the average treatment effect (ATE) using ChiRho."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "#!/usr/bin/env python\n# coding: utf-8\nimport torch\nimport pyro\nfrom pyro.infer.autoguide import AutoLowRankMultivariateNormal\nfrom pyro.infer import SVI, Trace_ELBO\nimport pytorch_lightning as pl\npyro.clear_param_store()\nclass LightningSVI(pl.LightningModule):\n    pass  # Placeholder for LightningSVI class definition\nbayesian_backdoor_sate = BayesianBackdoorSATE(BayesianBackdoor())  # Assuming previous class definitions\n\nclass LightningSVI(pl.LightningModule):\ndef __init__(self, elbo: pyro.infer.elbo.ELBOModule, **optim_params):\nsuper().__init__()\nself.optim_params = dict(optim_params)\nself.elbo = elbo\n\ndef configure_optimizers(self):\nreturn torch.optim.Adam(self.elbo.parameters(), **self.optim_params)\n\ndef training_step(self, batch, batch_idx):\nreturn self.elbo(*batch)\n\n\n# Fit a guide to the posterior distribution for the observed data only\nmodel = bayesian_backdoor_sate.conditioned_model\n\nguide = pyro.infer.autoguide.AutoLowRankMultivariateNormal(model)\nelbo = pyro.infer.Trace_ELBO(num_particles=100, vectorize_particles=True)\nelbo = elbo(model, guide)\n\n# initialize parameters\nelbo(covariates_obs, training_obs, earnings_obs)\n\n# fit parameters\nbatch_size = covariates_obs.shape[0]\ntrain_dataset = torch.utils.data.TensorDataset(covariates_obs, training_obs, earnings_obs)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\nsvi = LightningSVI(elbo, lr=0.1)\ntrainer = pl.Trainer(max_epochs=max_epochs, log_every_n_steps=1, accelerator=\"cpu\")", "description": "Using Pyro's SVI and PyTorch Lightning for fitting and approximating the posterior for causal inference."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "def plot_predictive(model, covariates_obs, training_obs, earnings_obs, guide=None, compare_source=True):\n    pass  # Placeholder\nimport pyro\nfrom pyro.infer.autoguide import AutoLowRankMultivariateNormal\npyro.clear_param_store()\nbayesian_backdoor = ConditionedBayesianBackdoor(BayesianBackdoor())  # Assuming previous definitions\nguide = AutoLowRankMultivariateNormal(bayesian_backdoor)  # Placeholder to represent guide\n\n# Visualize posterior predictive sample", "description": "Visualising posterior predictive samples to inspect the effects of the fitted model."}, {"origination_source_type": "doc_file", "origination_source": "/media/hdd/Code/beaker-bio/src/beaker_bio_context/chiro/documentation/backdoor.ipynb", "origination_method": "extract_from_library_automatic", "code": "import matplotlib.pyplot as plt\nimport pyro\nfrom pyro.infer import Predictive\npyro.clear_param_store()\nnum_samples = 10000  # Placeholder value\nnaive_prediction = 0  # Placeholder value, representing a naive estimate\nblog_prediction_ols = 1548.24 / 1000\nblog_prediction_matching = 1027.087 / 1000\nblog_prediction_matching_ci95 = [-705.131 / 1000, 2759.305 / 1000]\n\nfig, ax = plt.subplots(1, 1)\nax.hist(mf_prediction.squeeze(), bins=100, color=\"blue\", label=\"our estimate\")\nax.vlines(naive_prediction, 0, 50, color=\"red\", label=\"naive estimate\")\nax.vlines(blog_prediction_ols, 0, 50, color=\"green\", label=\"OLS estimate\")\nax.vlines(blog_prediction_matching, 0, 50, color=\"orange\", label=\"matching estimate\")\nax.vlines(blog_prediction_matching_ci95, 0, 50, color=\"orange\", linestyles=\"dashed\", label=\"matching estimate 95% confidence\")\nax.set(xlabel=\"ATE estimate\", ylabel=\"Frequency\")\nax.legend()\n\n\n# Here, we can clearly see that our approximate inference closely agrees with prior results using a simple linear regression and sample matching,\n# and all differ substantially from the naive estimate that simply ignores covariates altogether:\n# where the naive estimate finds a negative effect (i.e. participating in the training program reduced earnings on average!),\n# the estimates that control for covariates find a positive effect.\n#\n# Unfortunately for the job training program being studied, the more optimistic linear regression estimator does a poor job at controlling for covariates,\n# and as a result it is overconfident in its predictions on what is ultimately a very small and noisy dataset.\n# To illustrate this, we also include the 95% confidence interval for the matching estimate.\n# While this frequentist confidence interval is not directly comparable to our Bayesian posterior distribution,", "description": "Comparison of the probabilistic programming approach with linear regression and sample matching for causal inference."}]